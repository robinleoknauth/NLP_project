{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "\n",
    "# call glove2word2vec script\n",
    "# default way (through CLI): python -m gensim.scripts.glove2word2vec --input <glove_file> --output <w2v_file>\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "glove_file = datapath('/Users/robinleoknauth/downloads/glove/glove.6B.300d.txt')\n",
    "tmp_file = get_tmpfile(\"glove_word2vec.txt\")\n",
    "\n",
    "glove2word2vec(glove_file, tmp_file)\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.read_pickle(\"./data/df_merged_cleaned.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphanumeric = lambda x: re.sub('\\w*\\d\\w*', ' ', x)\n",
    "punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())\n",
    "clean_parens = lambda x: re.sub(r'\\([^)]+\\)', ' ', x)\n",
    "\n",
    "# df_merged['text'] = df_merged.transcript.map(clean_parens).map(punc_lower).map(alphanumeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['tokenized_text'] = df_merged['text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>description</th>\n",
       "      <th>duration</th>\n",
       "      <th>event</th>\n",
       "      <th>film_date</th>\n",
       "      <th>languages</th>\n",
       "      <th>main_speaker</th>\n",
       "      <th>name</th>\n",
       "      <th>num_speaker</th>\n",
       "      <th>published_date</th>\n",
       "      <th>ratings</th>\n",
       "      <th>related_talks</th>\n",
       "      <th>speaker_occupation</th>\n",
       "      <th>tags</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>views</th>\n",
       "      <th>transcript</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4553</td>\n",
       "      <td>Sir Ken Robinson makes an entertaining and pro...</td>\n",
       "      <td>1164</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>1140825600</td>\n",
       "      <td>60</td>\n",
       "      <td>Ken Robinson</td>\n",
       "      <td>Ken Robinson: Do schools kill creativity?</td>\n",
       "      <td>1</td>\n",
       "      <td>1151367060</td>\n",
       "      <td>[{'id': 7, 'name': 'Funny', 'count': 19645}, {...</td>\n",
       "      <td>[{'id': 865, 'hero': 'https://pe.tedcdn.com/im...</td>\n",
       "      <td>Author/educator</td>\n",
       "      <td>['children', 'creativity', 'culture', 'dance',...</td>\n",
       "      <td>Do schools kill creativity?</td>\n",
       "      <td>https://www.ted.com/talks/ken_robinson_says_sc...</td>\n",
       "      <td>47227110</td>\n",
       "      <td>Good morning. How are you?(Laughter)It's been ...</td>\n",
       "      <td>good morning  how are you  it s been great  ha...</td>\n",
       "      <td>[good, morning, how, are, you, it, s, been, gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>265</td>\n",
       "      <td>With the same humor and humanity he exuded in ...</td>\n",
       "      <td>977</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>1140825600</td>\n",
       "      <td>43</td>\n",
       "      <td>Al Gore</td>\n",
       "      <td>Al Gore: Averting the climate crisis</td>\n",
       "      <td>1</td>\n",
       "      <td>1151367060</td>\n",
       "      <td>[{'id': 7, 'name': 'Funny', 'count': 544}, {'i...</td>\n",
       "      <td>[{'id': 243, 'hero': 'https://pe.tedcdn.com/im...</td>\n",
       "      <td>Climate advocate</td>\n",
       "      <td>['alternative energy', 'cars', 'climate change...</td>\n",
       "      <td>Averting the climate crisis</td>\n",
       "      <td>https://www.ted.com/talks/al_gore_on_averting_...</td>\n",
       "      <td>3200520</td>\n",
       "      <td>Thank you so much, Chris. And it's truly a gre...</td>\n",
       "      <td>thank you so much  chris  and it s truly a gre...</td>\n",
       "      <td>[thank, you, so, much, chris, and, it, s, trul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>124</td>\n",
       "      <td>New York Times columnist David Pogue takes aim...</td>\n",
       "      <td>1286</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>1140739200</td>\n",
       "      <td>26</td>\n",
       "      <td>David Pogue</td>\n",
       "      <td>David Pogue: Simplicity sells</td>\n",
       "      <td>1</td>\n",
       "      <td>1151367060</td>\n",
       "      <td>[{'id': 7, 'name': 'Funny', 'count': 964}, {'i...</td>\n",
       "      <td>[{'id': 1725, 'hero': 'https://pe.tedcdn.com/i...</td>\n",
       "      <td>Technology columnist</td>\n",
       "      <td>['computers', 'entertainment', 'interface desi...</td>\n",
       "      <td>Simplicity sells</td>\n",
       "      <td>https://www.ted.com/talks/david_pogue_says_sim...</td>\n",
       "      <td>1636292</td>\n",
       "      <td>(Music: \"The Sound of Silence,\" Simon &amp; Garfun...</td>\n",
       "      <td>hello voice mail  my old friend  i ve called ...</td>\n",
       "      <td>[hello, voice, mail, my, old, friend, i, ve, c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   comments                                        description  duration  \\\n",
       "0      4553  Sir Ken Robinson makes an entertaining and pro...      1164   \n",
       "1       265  With the same humor and humanity he exuded in ...       977   \n",
       "2       124  New York Times columnist David Pogue takes aim...      1286   \n",
       "\n",
       "     event   film_date  languages  main_speaker  \\\n",
       "0  TED2006  1140825600         60  Ken Robinson   \n",
       "1  TED2006  1140825600         43       Al Gore   \n",
       "2  TED2006  1140739200         26   David Pogue   \n",
       "\n",
       "                                        name  num_speaker  published_date  \\\n",
       "0  Ken Robinson: Do schools kill creativity?            1      1151367060   \n",
       "1       Al Gore: Averting the climate crisis            1      1151367060   \n",
       "2              David Pogue: Simplicity sells            1      1151367060   \n",
       "\n",
       "                                             ratings  \\\n",
       "0  [{'id': 7, 'name': 'Funny', 'count': 19645}, {...   \n",
       "1  [{'id': 7, 'name': 'Funny', 'count': 544}, {'i...   \n",
       "2  [{'id': 7, 'name': 'Funny', 'count': 964}, {'i...   \n",
       "\n",
       "                                       related_talks    speaker_occupation  \\\n",
       "0  [{'id': 865, 'hero': 'https://pe.tedcdn.com/im...       Author/educator   \n",
       "1  [{'id': 243, 'hero': 'https://pe.tedcdn.com/im...      Climate advocate   \n",
       "2  [{'id': 1725, 'hero': 'https://pe.tedcdn.com/i...  Technology columnist   \n",
       "\n",
       "                                                tags  \\\n",
       "0  ['children', 'creativity', 'culture', 'dance',...   \n",
       "1  ['alternative energy', 'cars', 'climate change...   \n",
       "2  ['computers', 'entertainment', 'interface desi...   \n",
       "\n",
       "                         title  \\\n",
       "0  Do schools kill creativity?   \n",
       "1  Averting the climate crisis   \n",
       "2             Simplicity sells   \n",
       "\n",
       "                                                 url     views  \\\n",
       "0  https://www.ted.com/talks/ken_robinson_says_sc...  47227110   \n",
       "1  https://www.ted.com/talks/al_gore_on_averting_...   3200520   \n",
       "2  https://www.ted.com/talks/david_pogue_says_sim...   1636292   \n",
       "\n",
       "                                          transcript  \\\n",
       "0  Good morning. How are you?(Laughter)It's been ...   \n",
       "1  Thank you so much, Chris. And it's truly a gre...   \n",
       "2  (Music: \"The Sound of Silence,\" Simon & Garfun...   \n",
       "\n",
       "                                                text  \\\n",
       "0  good morning  how are you  it s been great  ha...   \n",
       "1  thank you so much  chris  and it s truly a gre...   \n",
       "2   hello voice mail  my old friend  i ve called ...   \n",
       "\n",
       "                                      tokenized_text  \n",
       "0  [good, morning, how, are, you, it, s, been, gr...  \n",
       "1  [thank, you, so, much, chris, and, it, s, trul...  \n",
       "2  [hello, voice, mail, my, old, friend, i, ve, c...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['tokenized_text'] = df_merged['tokenized_text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "\n",
    "df_merged['lemmatized_text'] = df_merged['tokenized_text'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>description</th>\n",
       "      <th>duration</th>\n",
       "      <th>event</th>\n",
       "      <th>film_date</th>\n",
       "      <th>languages</th>\n",
       "      <th>main_speaker</th>\n",
       "      <th>name</th>\n",
       "      <th>num_speaker</th>\n",
       "      <th>published_date</th>\n",
       "      <th>...</th>\n",
       "      <th>related_talks</th>\n",
       "      <th>speaker_occupation</th>\n",
       "      <th>tags</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>views</th>\n",
       "      <th>transcript</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>32</td>\n",
       "      <td>In an unmissable talk about race and politics ...</td>\n",
       "      <td>1100</td>\n",
       "      <td>TEDxMileHigh</td>\n",
       "      <td>1499472000</td>\n",
       "      <td>1</td>\n",
       "      <td>Theo E.J. Wilson</td>\n",
       "      <td>Theo E.J. Wilson: A black man goes undercover ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1506024042</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'id': 2512, 'hero': 'https://pe.tedcdn.com/i...</td>\n",
       "      <td>Public intellectual</td>\n",
       "      <td>['Internet', 'TEDx', 'United States', 'communi...</td>\n",
       "      <td>A black man goes undercover in the alt-right</td>\n",
       "      <td>https://www.ted.com/talks/theo_e_j_wilson_a_bl...</td>\n",
       "      <td>419309</td>\n",
       "      <td>I took a cell phone and accidentally made myse...</td>\n",
       "      <td>i took a cell phone and accidentally made myse...</td>\n",
       "      <td>[took, cell, phone, accidentally, made, famous...</td>\n",
       "      <td>[took, cell, phone, accidentally, made, famous...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>8</td>\n",
       "      <td>With more than half of the world population li...</td>\n",
       "      <td>519</td>\n",
       "      <td>TED2017</td>\n",
       "      <td>1492992000</td>\n",
       "      <td>1</td>\n",
       "      <td>Karoliina Korppoo</td>\n",
       "      <td>Karoliina Korppoo: How a video game might help...</td>\n",
       "      <td>1</td>\n",
       "      <td>1506092422</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'id': 2682, 'hero': 'https://pe.tedcdn.com/i...</td>\n",
       "      <td>Game designer</td>\n",
       "      <td>['cities', 'design', 'future', 'infrastructure...</td>\n",
       "      <td>How a video game might help us build better ci...</td>\n",
       "      <td>https://www.ted.com/talks/karoliina_korppoo_ho...</td>\n",
       "      <td>391721</td>\n",
       "      <td>We humans are becoming an urban species, so ci...</td>\n",
       "      <td>we humans are becoming an urban species  so ci...</td>\n",
       "      <td>[humans, becoming, urban, species, cities, nat...</td>\n",
       "      <td>[human, becoming, urban, specie, city, natural...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      comments                                        description  duration  \\\n",
       "2465        32  In an unmissable talk about race and politics ...      1100   \n",
       "2466         8  With more than half of the world population li...       519   \n",
       "\n",
       "             event   film_date  languages       main_speaker  \\\n",
       "2465  TEDxMileHigh  1499472000          1   Theo E.J. Wilson   \n",
       "2466       TED2017  1492992000          1  Karoliina Korppoo   \n",
       "\n",
       "                                                   name  num_speaker  \\\n",
       "2465  Theo E.J. Wilson: A black man goes undercover ...            1   \n",
       "2466  Karoliina Korppoo: How a video game might help...            1   \n",
       "\n",
       "      published_date  ...                                      related_talks  \\\n",
       "2465      1506024042  ...  [{'id': 2512, 'hero': 'https://pe.tedcdn.com/i...   \n",
       "2466      1506092422  ...  [{'id': 2682, 'hero': 'https://pe.tedcdn.com/i...   \n",
       "\n",
       "       speaker_occupation                                               tags  \\\n",
       "2465  Public intellectual  ['Internet', 'TEDx', 'United States', 'communi...   \n",
       "2466        Game designer  ['cities', 'design', 'future', 'infrastructure...   \n",
       "\n",
       "                                                  title  \\\n",
       "2465       A black man goes undercover in the alt-right   \n",
       "2466  How a video game might help us build better ci...   \n",
       "\n",
       "                                                    url   views  \\\n",
       "2465  https://www.ted.com/talks/theo_e_j_wilson_a_bl...  419309   \n",
       "2466  https://www.ted.com/talks/karoliina_korppoo_ho...  391721   \n",
       "\n",
       "                                             transcript  \\\n",
       "2465  I took a cell phone and accidentally made myse...   \n",
       "2466  We humans are becoming an urban species, so ci...   \n",
       "\n",
       "                                                   text  \\\n",
       "2465  i took a cell phone and accidentally made myse...   \n",
       "2466  we humans are becoming an urban species  so ci...   \n",
       "\n",
       "                                         tokenized_text  \\\n",
       "2465  [took, cell, phone, accidentally, made, famous...   \n",
       "2466  [humans, becoming, urban, species, cities, nat...   \n",
       "\n",
       "                                        lemmatized_text  \n",
       "2465  [took, cell, phone, accidentally, made, famous...  \n",
       "2466  [human, becoming, urban, specie, city, natural...  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "join lemmatized words to string for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['lemma_text_string'] = df_merged['lemmatized_text'].apply(', '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_topics_token = pipe.transform(df_merged['tokenized_text']) \n",
    "# df_topics_token = pd.DataFrame(df_topics_token, columns=[str(t_words[i]) for i in range(0,10)])\n",
    "# df_topics_token.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, number_top_words):\n",
    "        for ix, topic in enumerate(model.components_):\n",
    "            print(\"Topic: \", ix)\n",
    "            print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-number_top_words - 1:-1]]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_topics(model, feature_names, number_top_words):\n",
    "    result = list()\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "#         result.append(\" \".join([feature_names[i]\n",
    "#                     for i in topic.argsort()[:-number_top_words - 1:-1]]))\n",
    "        result.append([feature_names[i]\n",
    "                    for i in topic.argsort()[:-number_top_words - 1:-1]])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_topic_mod_lda(data = df_merged['lemma_text_string'], topics = 5,\n",
    "                       iters = 10, ngram_min = 1,\n",
    "                       ngram_max = 3, max_df=0.35,\n",
    "                       min_df = 0.1,\n",
    "                       max_feats=5000, number_top_words = 20,\n",
    "                       seed = 0):\n",
    "    \n",
    "    \"\"\" \n",
    "    vectorizer - turn words into numbers for each document\n",
    "    use Latent Dirichlet Allocation to produce topics\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    vectorizer = CountVectorizer(ngram_range = (ngram_min , ngram_max), \n",
    "                             stop_words ='english', \n",
    "                             max_df = max_df, \n",
    "                             max_features = max_feats)\n",
    "    \n",
    "#     vectorizer = TfidfVectorizer(stop_words=\"english\",\n",
    "# #                         use_idf=True,\n",
    "#                         ngram_range = (ngram_min , ngram_max),\n",
    "#                         min_df = min_df,\n",
    "                             \n",
    "#                         max_df = max_df,\n",
    "#                         max_features = max_feats,\n",
    "#                         )  \n",
    "    \n",
    "    \n",
    "    #  `fit (train), then transform` to convert text to a bag of words\n",
    "\n",
    "    vect_data = vectorizer.fit_transform(data)\n",
    "    \n",
    "    \n",
    "    \n",
    "    lda = LatentDirichletAllocation(n_components = topics,\n",
    "                                    max_iter = iters,\n",
    "                                    random_state = seed,\n",
    "                                    learning_method = 'online',\n",
    "                                    n_jobs =- 1,\n",
    "                                    )\n",
    "    \n",
    "    lda_data = lda.fit_transform(vect_data)\n",
    "    \n",
    "    display_topics(lda, vectorizer.get_feature_names(), number_top_words)\n",
    "    \n",
    "    return vectorizer, vect_data, lda, lda_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic  0\n",
      "water ocean earth sea foot surface ice planet mar robot fish air fly mile space cloud coral meter shark satellite\n",
      "Topic  1\n",
      "city design building space project create built build community street designer material public architecture house art live wall form york\n",
      "Topic  2\n",
      "africa african country aid south knowledge poor village continent leader nigeria poverty kenya region opportunity west community farmer east rural\n",
      "Topic  3\n",
      "car ca em road power drive hour vehicle mile driving yeah driver energy technology speed percent electric chris engine traffic\n",
      "Topic  4\n",
      "political group power society american social rule democracy believe community war culture movement muslim history reason moral election value wrong\n",
      "Topic  5\n",
      "child school kid student teacher education family parent learning learn class high old percent teach young college help girl classroom\n",
      "Topic  6\n",
      "company money dollar business percent market job product value cost million economy pay buy innovation industry example choice organization price\n",
      "Topic  7\n",
      "animal specie tree forest bird plant bee million vaccine nature creature dog virus insect live monkey male river mosquito organism\n",
      "Topic  8\n",
      "cell cancer dna gene body genome blood molecule tissue genetic disease protein tumor bacteria stem sugar lab biology organ drug\n",
      "Topic  9\n",
      "brain mind example pattern behavior body understand memory neuron study experience self turn information experiment science control person eye consciousness\n",
      "Topic  10\n",
      "universe planet light science star space earth physic theory particle galaxy number matter energy sun billion black hole telescope image\n",
      "Topic  11\n",
      "story man home wanted old took told saw family friend guy knew later night month happened father remember week moment\n",
      "Topic  12\n",
      "food water energy oil climate plant carbon percent fuel gas waste eat plastic billion nuclear farmer clean natural fish planet\n",
      "Topic  13\n",
      "data technology computer information machine using phone digital video device example online project tool algorithm real internet software open sort\n",
      "Topic  14\n",
      "play game music sound ok video oh yeah guy stuff hand robot sort try playing hear okay song audience feel\n",
      "Topic  15\n",
      "book story word language art read god write sort course writing english artist movie film century history painting wrote culture\n",
      "Topic  16\n",
      "woman love men feel girl experience live friend person man sex fear story family self feeling child relationship moment happy\n",
      "Topic  17\n",
      "patient health disease doctor care drug medical hospital treatment heart death medicine percent study surgery hiv research help blood trial\n",
      "Topic  18\n",
      "medium internet google web news facebook page twitter message site website search blog somebody link picture basically voice story couple\n",
      "Topic  19\n",
      "country government state percent global united china war million india public economic law nation number population growth chinese security policy\n"
     ]
    }
   ],
   "source": [
    "vectorizer_lda, vect_data, lda_model_lemma, lda_data_lemma = make_topic_mod_lda(data = df_merged['lemma_text_string'],\n",
    "                                    topics=20,\n",
    "                                    iters=100,\n",
    "                                    ngram_min=1, \n",
    "                                    ngram_max=1, \n",
    "                                    max_df=0.5, \n",
    "                                    min_df=0.1,\n",
    "                                    max_feats=2000\n",
    "                                                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.278040</td>\n",
       "      <td>0.024600</td>\n",
       "      <td>0.021508</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.072261</td>\n",
       "      <td>0.024452</td>\n",
       "      <td>0.208052</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.129159</td>\n",
       "      <td>0.110610</td>\n",
       "      <td>0.130402</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.038549</td>\n",
       "      <td>0.024020</td>\n",
       "      <td>0.097305</td>\n",
       "      <td>0.128625</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.188545</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.234207</td>\n",
       "      <td>0.232263</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.042473</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.012819</td>\n",
       "      <td>0.000108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.016996</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.137799</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.111167</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.414792</td>\n",
       "      <td>0.190844</td>\n",
       "      <td>0.073740</td>\n",
       "      <td>0.011755</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.025470</td>\n",
       "      <td>0.016641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.313576</td>\n",
       "      <td>0.048592</td>\n",
       "      <td>0.015511</td>\n",
       "      <td>0.059395</td>\n",
       "      <td>0.026528</td>\n",
       "      <td>0.107254</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.123169</td>\n",
       "      <td>0.143243</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.023341</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.138786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.132381</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.100770</td>\n",
       "      <td>0.037371</td>\n",
       "      <td>0.018579</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.129651</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.019522</td>\n",
       "      <td>0.011534</td>\n",
       "      <td>0.549311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.000083  0.000083  0.000083  0.000083  0.000083  0.278040  0.024600   \n",
       "1  0.000108  0.038549  0.024020  0.097305  0.128625  0.000108  0.188545   \n",
       "2  0.000072  0.000072  0.000072  0.016996  0.000072  0.000072  0.137799   \n",
       "3  0.000061  0.313576  0.048592  0.015511  0.059395  0.026528  0.107254   \n",
       "4  0.000073  0.000073  0.132381  0.000073  0.000073  0.100770  0.037371   \n",
       "\n",
       "         7         8         9         10        11        12        13  \\\n",
       "0  0.021508  0.000083  0.072261  0.024452  0.208052  0.000083  0.000083   \n",
       "1  0.000108  0.000108  0.000108  0.000108  0.234207  0.232263  0.000108   \n",
       "2  0.000072  0.000072  0.000072  0.000072  0.111167  0.000072  0.414792   \n",
       "3  0.000061  0.000061  0.000061  0.000061  0.123169  0.143243  0.000061   \n",
       "4  0.018579  0.000073  0.000073  0.000073  0.000073  0.000073  0.129651   \n",
       "\n",
       "         14        15        16        17        18        19  \n",
       "0  0.129159  0.110610  0.130402  0.000083  0.000083  0.000083  \n",
       "1  0.000108  0.042473  0.000108  0.000108  0.012819  0.000108  \n",
       "2  0.190844  0.073740  0.011755  0.000072  0.025470  0.016641  \n",
       "3  0.000061  0.000061  0.023341  0.000061  0.000061  0.138786  \n",
       "4  0.000073  0.000073  0.000073  0.019522  0.011534  0.549311  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(lda_data_lemma).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_ind = np.argmax(lda_data_lemma, axis=1)\n",
    "topic_ind.shape\n",
    "y = topic_ind\n",
    "\n",
    "# create text labels for plotting\n",
    "tsne_labels = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_names = tsne_labels\n",
    "# topic_names[topic_names==0] = \"History\"\n",
    "# topic_names[topic_names==1] = \"Medicine, Vaccines, Global Health\"\n",
    "# topic_names[topic_names==2] = \"Education\"\n",
    "# topic_names[topic_names==3] = \"Family\"\n",
    "# topic_names[topic_names==4] = \"Politics, War\"\n",
    "# topic_names[topic_names==5] = \"Technolgy\"\n",
    "# topic_names[topic_names==6] = \"Gender\"\n",
    "# topic_names[topic_names==7] = \"Astronomy, Quantum Physics\"\n",
    "# topic_names[topic_names==8] = \"Machine Learning, AI\"\n",
    "\n",
    "# topic_names[topic_names==9] = \"Gaming, Music, Video\"\n",
    "# topic_names[topic_names==10] = \"Tech, Business\"\n",
    "# topic_names[topic_names==11] = \"Biology, Genetics\"\n",
    "\n",
    "# topic_names[topic_names==12] = \"Medicine, Healthcare\"\n",
    "# topic_names[topic_names==13] = \"Energy, Transportation, Climate Change\"\n",
    "\n",
    "# topic_names[topic_names==14] = \"Astronomy, Space Travel\"\n",
    "# topic_names[topic_names==15] = \"Art, Language, Literature\"  \n",
    "# topic_names[topic_names==16] = \"Environmentalism, Oceans\"\n",
    "# topic_names[topic_names==17] = \"Mindfulness, Culture, Self Care\"\n",
    "# topic_names[topic_names==18] = \"Urban Dev, Architecture\"\n",
    "# topic_names[topic_names==19] = \"Economy, Global Econ, Development\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_lda = create_topics(lda_model_lemma, vectorizer_lda.get_feature_names(), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['water',\n",
       "  'ocean',\n",
       "  'earth',\n",
       "  'sea',\n",
       "  'foot',\n",
       "  'surface',\n",
       "  'ice',\n",
       "  'planet',\n",
       "  'mar',\n",
       "  'robot',\n",
       "  'fish',\n",
       "  'air',\n",
       "  'fly',\n",
       "  'mile',\n",
       "  'space',\n",
       "  'cloud',\n",
       "  'coral',\n",
       "  'meter',\n",
       "  'shark',\n",
       "  'satellite'],\n",
       " ['city',\n",
       "  'design',\n",
       "  'building',\n",
       "  'space',\n",
       "  'project',\n",
       "  'create',\n",
       "  'built',\n",
       "  'build',\n",
       "  'community',\n",
       "  'street',\n",
       "  'designer',\n",
       "  'material',\n",
       "  'public',\n",
       "  'architecture',\n",
       "  'house',\n",
       "  'art',\n",
       "  'live',\n",
       "  'wall',\n",
       "  'form',\n",
       "  'york'],\n",
       " ['africa',\n",
       "  'african',\n",
       "  'country',\n",
       "  'aid',\n",
       "  'south',\n",
       "  'knowledge',\n",
       "  'poor',\n",
       "  'village',\n",
       "  'continent',\n",
       "  'leader',\n",
       "  'nigeria',\n",
       "  'poverty',\n",
       "  'kenya',\n",
       "  'region',\n",
       "  'opportunity',\n",
       "  'west',\n",
       "  'community',\n",
       "  'farmer',\n",
       "  'east',\n",
       "  'rural'],\n",
       " ['car',\n",
       "  'ca',\n",
       "  'em',\n",
       "  'road',\n",
       "  'power',\n",
       "  'drive',\n",
       "  'hour',\n",
       "  'vehicle',\n",
       "  'mile',\n",
       "  'driving',\n",
       "  'yeah',\n",
       "  'driver',\n",
       "  'energy',\n",
       "  'technology',\n",
       "  'speed',\n",
       "  'percent',\n",
       "  'electric',\n",
       "  'chris',\n",
       "  'engine',\n",
       "  'traffic'],\n",
       " ['political',\n",
       "  'group',\n",
       "  'power',\n",
       "  'society',\n",
       "  'american',\n",
       "  'social',\n",
       "  'rule',\n",
       "  'democracy',\n",
       "  'believe',\n",
       "  'community',\n",
       "  'war',\n",
       "  'culture',\n",
       "  'movement',\n",
       "  'muslim',\n",
       "  'history',\n",
       "  'reason',\n",
       "  'moral',\n",
       "  'election',\n",
       "  'value',\n",
       "  'wrong'],\n",
       " ['child',\n",
       "  'school',\n",
       "  'kid',\n",
       "  'student',\n",
       "  'teacher',\n",
       "  'education',\n",
       "  'family',\n",
       "  'parent',\n",
       "  'learning',\n",
       "  'learn',\n",
       "  'class',\n",
       "  'high',\n",
       "  'old',\n",
       "  'percent',\n",
       "  'teach',\n",
       "  'young',\n",
       "  'college',\n",
       "  'help',\n",
       "  'girl',\n",
       "  'classroom'],\n",
       " ['company',\n",
       "  'money',\n",
       "  'dollar',\n",
       "  'business',\n",
       "  'percent',\n",
       "  'market',\n",
       "  'job',\n",
       "  'product',\n",
       "  'value',\n",
       "  'cost',\n",
       "  'million',\n",
       "  'economy',\n",
       "  'pay',\n",
       "  'buy',\n",
       "  'innovation',\n",
       "  'industry',\n",
       "  'example',\n",
       "  'choice',\n",
       "  'organization',\n",
       "  'price'],\n",
       " ['animal',\n",
       "  'specie',\n",
       "  'tree',\n",
       "  'forest',\n",
       "  'bird',\n",
       "  'plant',\n",
       "  'bee',\n",
       "  'million',\n",
       "  'vaccine',\n",
       "  'nature',\n",
       "  'creature',\n",
       "  'dog',\n",
       "  'virus',\n",
       "  'insect',\n",
       "  'live',\n",
       "  'monkey',\n",
       "  'male',\n",
       "  'river',\n",
       "  'mosquito',\n",
       "  'organism'],\n",
       " ['cell',\n",
       "  'cancer',\n",
       "  'dna',\n",
       "  'gene',\n",
       "  'body',\n",
       "  'genome',\n",
       "  'blood',\n",
       "  'molecule',\n",
       "  'tissue',\n",
       "  'genetic',\n",
       "  'disease',\n",
       "  'protein',\n",
       "  'tumor',\n",
       "  'bacteria',\n",
       "  'stem',\n",
       "  'sugar',\n",
       "  'lab',\n",
       "  'biology',\n",
       "  'organ',\n",
       "  'drug'],\n",
       " ['brain',\n",
       "  'mind',\n",
       "  'example',\n",
       "  'pattern',\n",
       "  'behavior',\n",
       "  'body',\n",
       "  'understand',\n",
       "  'memory',\n",
       "  'neuron',\n",
       "  'study',\n",
       "  'experience',\n",
       "  'self',\n",
       "  'turn',\n",
       "  'information',\n",
       "  'experiment',\n",
       "  'science',\n",
       "  'control',\n",
       "  'person',\n",
       "  'eye',\n",
       "  'consciousness'],\n",
       " ['universe',\n",
       "  'planet',\n",
       "  'light',\n",
       "  'science',\n",
       "  'star',\n",
       "  'space',\n",
       "  'earth',\n",
       "  'physic',\n",
       "  'theory',\n",
       "  'particle',\n",
       "  'galaxy',\n",
       "  'number',\n",
       "  'matter',\n",
       "  'energy',\n",
       "  'sun',\n",
       "  'billion',\n",
       "  'black',\n",
       "  'hole',\n",
       "  'telescope',\n",
       "  'image'],\n",
       " ['story',\n",
       "  'man',\n",
       "  'home',\n",
       "  'wanted',\n",
       "  'old',\n",
       "  'took',\n",
       "  'told',\n",
       "  'saw',\n",
       "  'family',\n",
       "  'friend',\n",
       "  'guy',\n",
       "  'knew',\n",
       "  'later',\n",
       "  'night',\n",
       "  'month',\n",
       "  'happened',\n",
       "  'father',\n",
       "  'remember',\n",
       "  'week',\n",
       "  'moment'],\n",
       " ['food',\n",
       "  'water',\n",
       "  'energy',\n",
       "  'oil',\n",
       "  'climate',\n",
       "  'plant',\n",
       "  'carbon',\n",
       "  'percent',\n",
       "  'fuel',\n",
       "  'gas',\n",
       "  'waste',\n",
       "  'eat',\n",
       "  'plastic',\n",
       "  'billion',\n",
       "  'nuclear',\n",
       "  'farmer',\n",
       "  'clean',\n",
       "  'natural',\n",
       "  'fish',\n",
       "  'planet'],\n",
       " ['data',\n",
       "  'technology',\n",
       "  'computer',\n",
       "  'information',\n",
       "  'machine',\n",
       "  'using',\n",
       "  'phone',\n",
       "  'digital',\n",
       "  'video',\n",
       "  'device',\n",
       "  'example',\n",
       "  'online',\n",
       "  'project',\n",
       "  'tool',\n",
       "  'algorithm',\n",
       "  'real',\n",
       "  'internet',\n",
       "  'software',\n",
       "  'open',\n",
       "  'sort'],\n",
       " ['play',\n",
       "  'game',\n",
       "  'music',\n",
       "  'sound',\n",
       "  'ok',\n",
       "  'video',\n",
       "  'oh',\n",
       "  'yeah',\n",
       "  'guy',\n",
       "  'stuff',\n",
       "  'hand',\n",
       "  'robot',\n",
       "  'sort',\n",
       "  'try',\n",
       "  'playing',\n",
       "  'hear',\n",
       "  'okay',\n",
       "  'song',\n",
       "  'audience',\n",
       "  'feel'],\n",
       " ['book',\n",
       "  'story',\n",
       "  'word',\n",
       "  'language',\n",
       "  'art',\n",
       "  'read',\n",
       "  'god',\n",
       "  'write',\n",
       "  'sort',\n",
       "  'course',\n",
       "  'writing',\n",
       "  'english',\n",
       "  'artist',\n",
       "  'movie',\n",
       "  'film',\n",
       "  'century',\n",
       "  'history',\n",
       "  'painting',\n",
       "  'wrote',\n",
       "  'culture'],\n",
       " ['woman',\n",
       "  'love',\n",
       "  'men',\n",
       "  'feel',\n",
       "  'girl',\n",
       "  'experience',\n",
       "  'live',\n",
       "  'friend',\n",
       "  'person',\n",
       "  'man',\n",
       "  'sex',\n",
       "  'fear',\n",
       "  'story',\n",
       "  'family',\n",
       "  'self',\n",
       "  'feeling',\n",
       "  'child',\n",
       "  'relationship',\n",
       "  'moment',\n",
       "  'happy'],\n",
       " ['patient',\n",
       "  'health',\n",
       "  'disease',\n",
       "  'doctor',\n",
       "  'care',\n",
       "  'drug',\n",
       "  'medical',\n",
       "  'hospital',\n",
       "  'treatment',\n",
       "  'heart',\n",
       "  'death',\n",
       "  'medicine',\n",
       "  'percent',\n",
       "  'study',\n",
       "  'surgery',\n",
       "  'hiv',\n",
       "  'research',\n",
       "  'help',\n",
       "  'blood',\n",
       "  'trial'],\n",
       " ['medium',\n",
       "  'internet',\n",
       "  'google',\n",
       "  'web',\n",
       "  'news',\n",
       "  'facebook',\n",
       "  'page',\n",
       "  'twitter',\n",
       "  'message',\n",
       "  'site',\n",
       "  'website',\n",
       "  'search',\n",
       "  'blog',\n",
       "  'somebody',\n",
       "  'link',\n",
       "  'picture',\n",
       "  'basically',\n",
       "  'voice',\n",
       "  'story',\n",
       "  'couple'],\n",
       " ['country',\n",
       "  'government',\n",
       "  'state',\n",
       "  'percent',\n",
       "  'global',\n",
       "  'united',\n",
       "  'china',\n",
       "  'war',\n",
       "  'million',\n",
       "  'india',\n",
       "  'public',\n",
       "  'economic',\n",
       "  'law',\n",
       "  'nation',\n",
       "  'number',\n",
       "  'population',\n",
       "  'growth',\n",
       "  'chinese',\n",
       "  'security',\n",
       "  'policy']]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>child school kid student teacher education fam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>story man home wanted old took told saw family...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data technology computer information machine u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>city design building space project create buil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>country government state percent global united...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>woman love men feel girl experience live frien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>story man home wanted old took told saw family...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  child school kid student teacher education fam...\n",
       "1  story man home wanted old took told saw family...\n",
       "2  data technology computer information machine u...\n",
       "3  city design building space project create buil...\n",
       "4  country government state percent global united...\n",
       "5  woman love men feel girl experience live frien...\n",
       "6  story man home wanted old took told saw family..."
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsne_labels.head(7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_whitespace = lambda x: re.sub('^\\s+|\\s+$|\\s+(?=\\s)', ' ', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_whitespace = lambda x: re.sub(' +', ' ', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>child school kid student teacher education fam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>story man home wanted old took told saw family...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data technology computer information machine u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>city design building space project create buil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>country government state percent global united...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  child school kid student teacher education fam...\n",
       "1  story man home wanted old took told saw family...\n",
       "2  data technology computer information machine u...\n",
       "3  city design building space project create buil...\n",
       "4  country government state percent global united..."
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_names = tsne_labels\n",
    "\n",
    "for i in list( range(0, len(topics_lda))):\n",
    "    topic_names[topic_names == i] = ' '.join(topics_lda[ i ])\n",
    "\n",
    "\n",
    "topic_names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_names['topic_names'] = topic_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_names['topic_names'] = topic_names.topic_names.map(remove_whitespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_names.to_pickle('./data/topic_names.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       ['children', 'creativity', 'culture', 'dance',...\n",
       "1       ['alternative energy', 'cars', 'climate change...\n",
       "2       ['computers', 'entertainment', 'interface desi...\n",
       "3       ['MacArthur grant', 'activism', 'business', 'c...\n",
       "4       ['Africa', 'Asia', 'Google', 'demo', 'economic...\n",
       "5       ['business', 'culture', 'entertainment', 'goal...\n",
       "6       ['Christianity', 'God', 'atheism', 'comedy', '...\n",
       "7       ['architecture', 'collaboration', 'culture', '...\n",
       "8       ['God', 'TED Brain Trust', 'atheism', 'brain',...\n",
       "9       ['Christianity', 'God', 'culture', 'happiness'...\n",
       "10      ['activism', 'architecture', 'collaboration', ...\n",
       "11      ['TED Prize', 'art', 'culture', 'entertainment...\n",
       "12      ['TED Prize', 'collaboration', 'disease', 'ebo...\n",
       "13      ['demo', 'design', 'interface design', 'techno...\n",
       "14      ['children', 'design', 'education', 'entrepren...\n",
       "15      ['entertainment', 'music', 'performance', 'vio...\n",
       "16      ['creativity', 'entertainment', 'music', 'perf...\n",
       "17      ['MacArthur grant', 'alternative energy', 'des...\n",
       "18      ['DNA', 'biology', 'creativity', 'design', 'in...\n",
       "19      ['business', 'collaboration', 'culture', 'inve...\n",
       "20      ['business', 'collaboration', 'culture', 'glob...\n",
       "21      ['collaboration', 'comedy', 'community', 'cult...\n",
       "22      ['business', 'communication', 'community', 'cu...\n",
       "23      ['cognitive science', 'culture', 'evolution', ...\n",
       "24      ['culture', 'entertainment', 'gender', 'global...\n",
       "25      ['climate change', 'cosmos', 'culture', 'envir...\n",
       "26      ['astronomy', 'biology', 'cognitive science', ...\n",
       "27      ['business', 'cities', 'culture', 'economics',...\n",
       "28      ['business', 'choice', 'consumerism', 'culture...\n",
       "29      ['TED Brain Trust', 'brain', 'choice', 'cultur...\n",
       "                              ...                        \n",
       "2437    ['AI', 'computers', 'economics', 'future', 'hu...\n",
       "2438    ['biology', 'biomechanics', 'cancer', 'chemist...\n",
       "2439    ['Africa', 'art', 'beauty', 'creativity', 'per...\n",
       "2440    ['TEDx', 'business', 'capitalism', 'community'...\n",
       "2441    ['Africa', 'TED Fellows', 'art', 'creativity',...\n",
       "2442    ['TEDx', 'activism', 'children', 'family', 'po...\n",
       "2443    ['AI', 'education', 'intelligence', 'robots', ...\n",
       "2444    ['TEDx', 'United States', 'activism', 'democra...\n",
       "2445    ['TEDx', 'ancient world', 'archaeology', 'cons...\n",
       "2446    ['TEDx', 'community', 'humanity', 'identity', ...\n",
       "2447    ['architecture', 'art', 'cities', 'creativity'...\n",
       "2448    ['TEDx', 'security', 'social media', 'terroris...\n",
       "2449    ['algorithm', 'business', 'collaboration', 'co...\n",
       "2450    ['TED en EspaÃ±ol', 'art', 'creativity', 'desig...\n",
       "2451    ['TEDx', 'art', 'climate change', 'environment...\n",
       "2452    ['Africa', 'activism', 'cities', 'government',...\n",
       "2453    ['TEDx', 'communication', 'friendship', 'polit...\n",
       "2454    ['happiness', 'humanity', 'personal growth', '...\n",
       "2455    ['TEDx', 'business', 'corruption', 'economics'...\n",
       "2456    ['Africa', 'agriculture', 'farming', 'food', '...\n",
       "2457    ['business', 'capitalism', 'collaboration', 'e...\n",
       "2458               ['TEDx', 'physics', 'play', 'science']\n",
       "2459    ['Africa', 'activism', 'art', 'community', 'hi...\n",
       "2460    ['DNA', 'Human body', 'biology', 'data', 'entr...\n",
       "2461    ['Africa', 'agriculture', 'history', 'leadersh...\n",
       "2462    ['TED Residency', 'United States', 'community'...\n",
       "2463    ['Mars', 'South America', 'TED Fellows', 'astr...\n",
       "2464    ['AI', 'ants', 'fish', 'future', 'innovation',...\n",
       "2465    ['Internet', 'TEDx', 'United States', 'communi...\n",
       "2466    ['cities', 'design', 'future', 'infrastructure...\n",
       "Name: tags, Length: 2467, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.7916e-01, -1.2207e-01, -2.2908e-01,  3.4682e-01,  3.0810e-01,\n",
       "       -4.8354e-01, -1.2872e-01, -4.6677e-02, -3.6068e-02, -2.1610e+00,\n",
       "        2.7579e-01,  1.1901e-01,  1.6028e-01, -6.1450e-02,  1.1554e-01,\n",
       "       -3.5981e-01,  4.8871e-01,  7.9839e-02,  3.3019e-02,  2.9834e-01,\n",
       "       -9.9063e-02,  6.6332e-01,  3.3691e-01,  5.7280e-02, -2.9128e-01,\n",
       "        2.7535e-01,  5.6892e-01,  2.5413e-01, -4.3233e-02,  2.0520e-02,\n",
       "        5.0779e-02,  5.4505e-01, -4.3760e-01,  1.0415e-01, -6.3288e-01,\n",
       "        1.4536e-01,  3.9711e-01,  4.3029e-01, -1.2240e-01, -6.6137e-02,\n",
       "        1.0925e-01, -5.0684e-01,  3.9058e-01, -5.6568e-01, -4.3140e-01,\n",
       "       -1.4120e-01, -1.9887e-01,  3.1592e-01, -3.6014e-01, -4.1200e-02,\n",
       "        6.4403e-02,  8.0507e-02, -1.7772e-02, -6.5428e-01,  1.6071e-01,\n",
       "       -3.9521e-01,  1.4980e-01, -5.0789e-01, -4.1719e-01,  4.7289e-02,\n",
       "        4.4494e-01, -2.7687e-01,  2.2294e-01,  1.7580e-02, -2.1816e-01,\n",
       "       -3.1820e-01,  2.9193e-01,  6.4577e-01,  2.4003e-01, -3.7022e-01,\n",
       "       -5.5074e-01, -1.2535e-01, -1.5050e-01, -3.2044e-01,  5.8076e-02,\n",
       "       -2.6141e-01, -2.6515e-01,  2.6757e-01,  2.0522e-01,  1.1733e-01,\n",
       "       -6.2157e-02, -2.2650e-01,  3.2636e-01, -4.0547e-01,  2.4161e-01,\n",
       "       -2.5849e-01,  2.8644e-01,  2.3234e-01, -4.5893e-01,  1.5047e-01,\n",
       "       -2.5702e-01,  1.9053e-01, -1.8505e-01,  2.9386e-01,  5.3166e-01,\n",
       "       -4.1142e-01,  5.6568e-01,  7.1885e-02,  5.3228e-02, -7.7315e-01,\n",
       "        4.3245e-01, -4.2197e-01, -6.7942e-01, -3.4763e-01,  5.5123e-02,\n",
       "        2.1531e-01, -4.5282e-01,  4.2466e-02, -1.3439e-03, -5.1621e-01,\n",
       "        3.8041e-01, -7.6564e-01,  4.2542e-01,  1.8234e-01,  5.2484e-01,\n",
       "       -4.6742e-02,  4.2889e-01,  1.2920e-01, -6.1711e-01, -2.6509e-01,\n",
       "        3.4117e-01,  8.1478e-01,  1.0115e-01,  3.0683e-01,  3.1813e-01,\n",
       "       -2.4696e-01,  4.0815e-01,  1.3617e-01,  4.1390e-01, -2.0441e-01,\n",
       "       -3.5409e-02, -2.2744e-01,  1.5432e-01, -2.0123e-01, -3.6867e-01,\n",
       "       -6.2436e-01,  5.9698e-03,  2.7608e-01, -4.6925e-02,  6.6007e-01,\n",
       "       -6.5450e-01,  5.6801e-01, -8.3432e-02,  3.0107e-01,  9.9073e-01,\n",
       "        3.1901e-01,  4.4961e-01,  3.1485e-01, -1.3806e-01,  3.0476e-01,\n",
       "        7.6140e-01,  1.5937e-01, -1.5931e-01, -5.3339e-02, -3.5352e-01,\n",
       "        1.7250e-01,  3.2794e-01,  1.8236e-01,  1.5218e-01, -3.3884e-01,\n",
       "        8.7456e-03,  4.4977e-01,  3.8210e-01, -8.0203e-01, -4.3643e-01,\n",
       "       -5.1440e-02, -2.3985e-01,  1.4032e-01,  4.5397e-01,  1.7427e-01,\n",
       "       -3.2304e-02, -3.4719e-01, -8.6875e-01,  2.5163e-01,  4.1114e-01,\n",
       "       -3.7293e-01,  1.3172e-01,  4.6399e-02, -3.9607e-02,  7.1769e-01,\n",
       "        5.4314e-01,  1.2344e-01,  5.0076e-01, -1.0772e-01, -5.2271e-01,\n",
       "        6.7043e-02, -4.1180e-01, -7.4812e-02, -1.7089e-01, -2.7040e-01,\n",
       "       -8.8300e-02,  2.3628e-02, -5.0854e-01,  1.5211e-01, -3.0964e-01,\n",
       "        8.4908e-01,  4.6413e-02,  1.6211e-01,  4.6980e-01,  1.7594e-01,\n",
       "        1.7096e-01, -1.0946e-01,  1.6183e-01,  4.4360e-01, -2.5097e-01,\n",
       "       -3.3531e-01,  6.6059e-02, -2.6647e-01,  3.3417e-01,  6.3775e-01,\n",
       "        6.2871e-02, -3.2834e-01,  1.7064e-01, -1.8338e-01,  3.6736e-02,\n",
       "        4.3476e-01,  1.9022e-01,  3.3518e-01,  8.0531e-01,  6.5139e-01,\n",
       "       -3.4491e-01,  8.0955e-01,  4.7919e-02,  1.0221e-01,  7.3825e-02,\n",
       "       -3.7203e-01, -1.8671e-01, -8.2490e-03, -2.1370e-02,  1.7367e-01,\n",
       "       -1.8885e-01,  5.2529e-02,  1.6441e-01, -1.5935e-02, -2.3422e-01,\n",
       "        1.3141e-01,  4.3711e-03,  6.5476e-03, -1.1993e-01,  4.9854e-02,\n",
       "        2.5672e-01,  2.5477e-02,  1.1040e+00, -1.3283e-01, -1.4002e+00,\n",
       "        6.1265e-01,  3.7408e-01, -1.0010e-01,  2.2123e-01,  2.1231e-01,\n",
       "        1.6619e-02,  1.9977e-01,  5.3672e-01,  1.4011e-01,  1.4122e-01,\n",
       "        2.3139e-01,  2.8986e-02, -2.7415e-01,  1.4629e-02,  1.9029e-01,\n",
       "        4.3212e-02, -5.4435e-01,  8.3612e-01, -2.9038e-01, -2.3002e-01,\n",
       "        2.3775e-02, -4.7197e-01,  5.2886e-01,  6.1372e-02,  1.7747e-01,\n",
       "       -3.2086e-01,  5.7877e-01, -1.3479e-03,  1.5321e-01,  2.5178e-01,\n",
       "       -2.3725e-01, -1.6931e+00, -3.8129e-02,  1.2742e+00, -3.9827e-01,\n",
       "       -1.5659e-01,  1.3184e-01,  3.9775e-02, -2.3194e-01,  1.8847e-01,\n",
       "       -3.4044e-02, -1.2303e-01,  9.3973e-02,  3.0218e-01,  2.3077e-01,\n",
       "       -1.5970e-01, -1.6160e-01, -2.7032e-01,  1.0683e-02,  1.6078e-01,\n",
       "        3.7757e-01, -2.1858e-01,  2.7425e-02, -2.8656e-01, -3.0284e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[topics_lda[0][0]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['children', 'creativity', 'culture', 'dance', 'education', 'parenting', 'teaching']\""
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.iloc[0].tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = df_merged.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tags = tags.str.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = tags.apply(punc_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = tags.apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = tags.apply(lambda x: re.sub('^\\s+|\\s+$', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = tags.apply(lambda x: re.sub(' +', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = tags.apply(lambda x: x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['children',\n",
       " 'creativity',\n",
       " 'culture',\n",
       " 'dance',\n",
       " 'education',\n",
       " 'parenting',\n",
       " 'teaching']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove words from tags that are not in word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list( range(0, 2467) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in l:\n",
    "    tags[i] = [word for word in model.vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate centroids for the tags\n",
    "\n",
    "NOTE: This might take a long time. With a 2017 MacBood Pro it took around 3.5 hours. You might want to just import the pickle instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centroids = list()\n",
    "# for i in l:\n",
    "#     centroids.append(np.mean(model[tags[i]], axis=0))\n",
    "    \n",
    "# centroids = np.array(centroids)\n",
    "# centroids = centroids.reshape((2467, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.2382111e-02, -8.3333418e-02, -4.7429472e-05, ...,\n",
       "         2.1872786e-01,  1.8184374e-01, -6.7022957e-02],\n",
       "       [ 9.2382111e-02, -8.3333418e-02, -4.7429472e-05, ...,\n",
       "         2.1872786e-01,  1.8184374e-01, -6.7022957e-02],\n",
       "       [ 9.2382111e-02, -8.3333418e-02, -4.7429472e-05, ...,\n",
       "         2.1872786e-01,  1.8184374e-01, -6.7022957e-02],\n",
       "       ...,\n",
       "       [ 9.2382111e-02, -8.3333418e-02, -4.7429472e-05, ...,\n",
       "         2.1872786e-01,  1.8184374e-01, -6.7022957e-02],\n",
       "       [ 9.2382111e-02, -8.3333418e-02, -4.7429472e-05, ...,\n",
       "         2.1872786e-01,  1.8184374e-01, -6.7022957e-02],\n",
       "       [ 9.2382111e-02, -8.3333418e-02, -4.7429472e-05, ...,\n",
       "         2.1872786e-01,  1.8184374e-01, -6.7022957e-02]], dtype=float32)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = pd.DataFrame(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centroids.to_pickle('./data/centrodis_tags.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate centroids for the topics of LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_topics_lda = list( range(0, len(topics_lda)))\n",
    "topic_centroids = list()\n",
    "for i in len_topics_lda:\n",
    "    topic_centroids.append(np.mean(model[topics_lda[i]], axis=0))\n",
    "    \n",
    "topic_centroids = np.array(topic_centroids)\n",
    "topic_centroids = topic_centroids.reshape((len(len_topics_lda), 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.91942956e-02, -2.94518527e-02, -1.77461311e-01, -1.98520631e-01,\n",
       "       -1.79088175e-01,  1.19112037e-01,  9.54154693e-03,  1.51460171e-01,\n",
       "        1.85122803e-01, -1.14885700e+00,  3.40971947e-01, -9.17031989e-02,\n",
       "        4.19294536e-02, -9.66227502e-02,  1.39629720e-02,  1.98253140e-01,\n",
       "        9.09077935e-03,  2.91654259e-01, -1.43830642e-01,  4.42994982e-01,\n",
       "       -2.54677534e-01,  1.33470193e-01, -4.68761101e-02,  2.79659092e-01,\n",
       "        5.14540598e-02,  9.01829079e-02,  7.17936605e-02,  2.07484007e-01,\n",
       "       -2.74153322e-01,  1.41550899e-01,  1.91401213e-01,  1.42637357e-01,\n",
       "       -4.24818814e-01, -2.66560435e-01, -5.92478514e-02,  1.03942834e-01,\n",
       "        8.48085620e-03, -1.21008590e-01, -3.17262001e-02,  4.37371075e-01,\n",
       "       -2.69010752e-01,  1.46936670e-01,  1.06667958e-01,  2.61291325e-01,\n",
       "       -1.68888390e-01,  9.48779956e-02,  3.81530732e-01,  1.32891461e-01,\n",
       "        1.67832226e-01, -6.17890060e-02,  8.38654581e-03,  5.88715682e-03,\n",
       "       -3.20000462e-02, -7.38370568e-02, -2.53638513e-02,  1.93281263e-01,\n",
       "        5.72532527e-02,  8.96936506e-02,  9.11603123e-02,  1.09127402e-01,\n",
       "       -1.83139607e-01,  1.87994003e-01,  4.40762460e-01, -1.38169318e-01,\n",
       "        5.13630919e-02, -6.95912093e-02, -1.95463985e-01,  1.72260463e-01,\n",
       "       -1.21808246e-01,  1.14064962e-02,  2.10266680e-01,  1.67861089e-01,\n",
       "       -4.47321460e-02, -6.85394630e-02, -3.10412049e-01,  1.34138957e-01,\n",
       "        1.77554488e-01, -7.94569999e-02,  1.03554949e-01, -1.26207501e-01,\n",
       "        1.21228948e-01, -1.17800906e-01, -2.37614706e-01,  4.84482534e-02,\n",
       "       -1.89510614e-01,  2.57654376e-02,  1.07143417e-01,  9.21829045e-02,\n",
       "        1.87190250e-03, -3.19676340e-01,  4.16134000e-02,  6.93886057e-02,\n",
       "        7.85202011e-02, -1.53543904e-01, -1.54451996e-01,  8.15926939e-02,\n",
       "       -8.60447884e-02,  9.35759023e-02,  8.63326043e-02, -2.01391384e-01,\n",
       "        4.05012369e-02,  1.81451350e-01, -4.00215387e-03, -6.89963028e-02,\n",
       "        5.55726588e-02,  1.97957844e-01,  2.03359753e-01, -5.76667003e-02,\n",
       "       -2.49455515e-02,  1.24873482e-02, -1.30997911e-01, -1.86477304e-01,\n",
       "       -8.85115378e-03, -8.67418498e-02, -1.65877461e-01, -6.63571954e-02,\n",
       "       -2.77769868e-03,  1.71655789e-01, -4.19686995e-02, -1.04121640e-01,\n",
       "       -7.30563849e-02, -2.55392820e-01, -1.96537022e-02,  1.02971092e-01,\n",
       "       -1.15543522e-01,  2.44575500e-01, -7.90335983e-02,  2.04685971e-01,\n",
       "        1.65405974e-01, -4.73477468e-02, -1.10547997e-01,  2.98704922e-01,\n",
       "        1.98612794e-01,  2.85354316e-01,  1.46079108e-01, -1.13521636e-01,\n",
       "        6.52669892e-02,  3.10834553e-02, -2.44496658e-01,  6.48757070e-02,\n",
       "        6.69342503e-02, -2.41324976e-02,  3.71958390e-02, -3.47693637e-02,\n",
       "       -3.91794294e-01, -2.77080480e-02,  5.70194907e-02,  2.74307966e-01,\n",
       "       -7.51250535e-02, -4.51251939e-02,  3.20936948e-01,  2.93065123e-02,\n",
       "       -1.16780080e-01, -1.04242466e-01,  6.10166609e-01, -2.07215007e-02,\n",
       "       -5.02223000e-02, -5.77705503e-02,  3.22257802e-02, -3.24929096e-02,\n",
       "        3.79585512e-02, -3.19755614e-01,  2.51687676e-01, -7.28273988e-02,\n",
       "        3.10605705e-01, -1.91281527e-01,  9.65502411e-02,  2.24514201e-01,\n",
       "       -1.77404042e-02,  2.88198948e-01, -1.05964459e-01,  1.30805047e-02,\n",
       "        1.33359507e-01,  8.28312486e-02, -6.12535104e-02, -3.25916559e-02,\n",
       "       -6.98473975e-02,  1.40613049e-01,  7.98189491e-02, -2.88955986e-01,\n",
       "       -2.35347617e-02, -3.42146531e-02,  9.37455669e-02,  5.20208478e-02,\n",
       "        7.89437518e-02, -4.81100738e-01,  4.36797529e-01,  1.45406455e-01,\n",
       "        3.68588455e-02, -2.11096052e-02,  2.04418212e-01,  3.05301487e-01,\n",
       "       -6.15617819e-02, -9.52126384e-02,  5.29438034e-02,  1.27156571e-01,\n",
       "        1.20794447e-02, -1.61962897e-01, -9.33511481e-02, -1.02950484e-02,\n",
       "        9.95522618e-01,  4.47838530e-02, -1.11359134e-01, -7.64770508e-02,\n",
       "        6.25409037e-02,  1.63257092e-01, -1.29925251e-01, -1.99265614e-01,\n",
       "       -3.18176970e-02,  1.15625776e-01, -1.06860772e-01, -1.44904852e-01,\n",
       "        4.83431444e-02, -3.67229640e-01,  2.45648455e-02, -5.62485680e-02,\n",
       "        1.86444789e-01, -1.18507147e-01,  2.71057133e-02, -8.71527269e-02,\n",
       "        4.88805234e-01, -1.25690605e-02, -4.15434986e-02, -5.33360103e-03,\n",
       "       -2.46230420e-02,  4.28147279e-02, -1.11097395e-01, -3.46869141e-01,\n",
       "       -1.57307506e-01,  9.72623825e-02,  3.97620380e-01,  1.33341089e-01,\n",
       "       -7.31712356e-02, -2.35991240e-01,  1.59564555e-01, -5.46037555e-02,\n",
       "       -1.62228122e-01, -1.08889556e-02,  4.44978960e-02, -2.25368794e-03,\n",
       "       -1.42424390e-01,  8.12966526e-02, -1.47267934e-02, -2.14027047e-01,\n",
       "       -5.38254976e-01, -2.93421037e-02,  3.18509117e-02,  1.86610427e-02,\n",
       "       -5.88213094e-02,  2.14713253e-02, -1.02782249e-01,  1.76229075e-01,\n",
       "       -1.72543414e-02, -3.24446037e-02,  1.09295569e-01,  1.46420151e-01,\n",
       "        6.51314715e-03, -1.90440804e-01,  7.49616474e-02,  1.27917305e-01,\n",
       "        1.58748075e-01, -2.03549594e-01, -1.11829065e-01, -1.81109458e-01,\n",
       "        1.73022598e-03, -1.18893817e-01, -7.67128468e-02,  5.12193516e-02,\n",
       "        2.44537264e-01,  1.82900026e-01,  7.43731100e-04,  1.45123035e-01,\n",
       "        9.88044776e-03,  1.34736538e-01,  1.56816300e-02,  2.48038061e-02,\n",
       "       -1.41651797e+00,  1.09559596e-01, -2.60935634e-01, -2.10194543e-01,\n",
       "       -3.16831142e-01,  2.48519495e-01,  3.16085806e-03,  7.30577931e-02,\n",
       "       -2.59025663e-01, -6.88002855e-02, -2.30573058e-01, -8.66662040e-02,\n",
       "        1.44378752e-01,  2.89989058e-02, -9.64432955e-02,  2.56109178e-01,\n",
       "       -2.53828615e-01, -1.02748834e-01,  1.31258788e-02, -5.75335026e-02,\n",
       "        1.07562400e-01, -7.75815174e-02, -3.27306613e-02, -1.52781159e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_centroids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ocean', 0.7241881489753723),\n",
       " ('sea', 0.6944864988327026),\n",
       " ('surface', 0.6720656156539917),\n",
       " ('earth', 0.6614603400230408),\n",
       " ('water', 0.6562643647193909)]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[topic_centroids[0]], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = list( range(0, len(topic_centroids)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ocean', 0.7241881489753723), ('sea', 0.6944864988327026), ('surface', 0.6720656156539917)]\n",
      "[('building', 0.7508234977722168), ('new', 0.6651170253753662), ('built', 0.6584510803222656)]\n",
      "[('africa', 0.7412379384040833), ('country', 0.7243189811706543), ('african', 0.7052187919616699)]\n",
      "[('car', 0.7246485948562622), ('driving', 0.7131131887435913), ('vehicle', 0.677789568901062)]\n",
      "[('political', 0.7203155755996704), ('that', 0.6875946521759033), ('what', 0.6868358850479126)]\n",
      "[('school', 0.7647520899772644), ('students', 0.7405155897140503), ('teacher', 0.7326071262359619)]\n",
      "[('cost', 0.7275770306587219), ('companies', 0.7208005785942078), ('market', 0.7180787324905396)]\n",
      "[('animal', 0.7121767401695251), ('animals', 0.7056883573532104), ('bird', 0.6857052445411682)]\n",
      "[('cells', 0.7295016646385193), ('genetic', 0.7097228169441223), ('genes', 0.7032840251922607)]\n",
      "[('kind', 0.6999107003211975), ('how', 0.697600245475769), ('mind', 0.6975491046905518)]\n",
      "[('earth', 0.7260396480560303), ('planet', 0.6780874729156494), ('space', 0.6427775025367737)]\n",
      "[('when', 0.7775875329971313), ('what', 0.754043459892273), ('came', 0.7447168827056885)]\n",
      "[('gas', 0.6982955932617188), ('fuel', 0.6949659585952759), ('water', 0.6892861127853394)]\n",
      "[('computer', 0.7741423845291138), ('software', 0.7575250267982483), ('internet', 0.7549252510070801)]\n",
      "[('you', 0.7940981388092041), ('really', 0.7811866998672485), ('me', 0.7508089542388916)]\n",
      "[('writing', 0.7476145029067993), ('book', 0.7328019142150879), ('written', 0.6729103326797485)]\n",
      "[('love', 0.7249577045440674), ('kind', 0.7205901145935059), ('know', 0.7153167724609375)]\n",
      "[('patients', 0.7884446382522583), ('medical', 0.7648485898971558), ('doctors', 0.7408198118209839)]\n",
      "[('web', 0.8325105905532837), ('internet', 0.7640730738639832), ('online', 0.7212302684783936)]\n",
      "[('country', 0.7351666688919067), ('government', 0.7246681451797485), ('nation', 0.6920499205589294)]\n"
     ]
    }
   ],
   "source": [
    "for i in le:\n",
    "    print(model.most_similar(positive=[topic_centroids[i]], topn = 3))\n",
    "    topic_names.append(model.most_similar(positive=[topic_centroids[i]], topn = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the lda_model as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_filepath_model = \"./data/lda_model.pkl\"  \n",
    "with open(pkl_filepath_model, 'wb') as file:  \n",
    "    pickle.dump(lda_model_lemma, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_filepath_lda_data = \"./data/lda_model_data.pkl\"  \n",
    "with open(pkl_filepath_lda_data, 'wb') as file:  \n",
    "    pickle.dump(lda_data_lemma, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_filepath_vectorizer = \"./data/vectorizer.pkl\"  \n",
    "with open(pkl_filepath_vectorizer, 'wb') as file:  \n",
    "    pickle.dump(vectorizer_lda, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
