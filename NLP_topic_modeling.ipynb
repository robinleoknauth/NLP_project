{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "\n",
    "# call glove2word2vec script\n",
    "# default way (through CLI): python -m gensim.scripts.glove2word2vec --input <glove_file> --output <w2v_file>\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "glove_file = datapath('/Users/robinleoknauth/downloads/glove/glove.6B.300d.txt')\n",
    "tmp_file = get_tmpfile(\"glove_word2vec.txt\")\n",
    "\n",
    "glove2word2vec(glove_file, tmp_file)\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.read_pickle(\"./data/df_merged_cleaned.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphanumeric = lambda x: re.sub('\\w*\\d\\w*', ' ', x)\n",
    "punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())\n",
    "clean_parens = lambda x: re.sub(r'\\([^)]+\\)', ' ', x)\n",
    "\n",
    "# df_merged['text'] = df_merged.transcript.map(clean_parens).map(punc_lower).map(alphanumeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['tokenized_text'] = df_merged['text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>description</th>\n",
       "      <th>duration</th>\n",
       "      <th>event</th>\n",
       "      <th>film_date</th>\n",
       "      <th>languages</th>\n",
       "      <th>main_speaker</th>\n",
       "      <th>name</th>\n",
       "      <th>num_speaker</th>\n",
       "      <th>published_date</th>\n",
       "      <th>ratings</th>\n",
       "      <th>related_talks</th>\n",
       "      <th>speaker_occupation</th>\n",
       "      <th>tags</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>views</th>\n",
       "      <th>transcript</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4553</td>\n",
       "      <td>Sir Ken Robinson makes an entertaining and pro...</td>\n",
       "      <td>1164</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>1140825600</td>\n",
       "      <td>60</td>\n",
       "      <td>Ken Robinson</td>\n",
       "      <td>Ken Robinson: Do schools kill creativity?</td>\n",
       "      <td>1</td>\n",
       "      <td>1151367060</td>\n",
       "      <td>[{'id': 7, 'name': 'Funny', 'count': 19645}, {...</td>\n",
       "      <td>[{'id': 865, 'hero': 'https://pe.tedcdn.com/im...</td>\n",
       "      <td>Author/educator</td>\n",
       "      <td>['children', 'creativity', 'culture', 'dance',...</td>\n",
       "      <td>Do schools kill creativity?</td>\n",
       "      <td>https://www.ted.com/talks/ken_robinson_says_sc...</td>\n",
       "      <td>47227110</td>\n",
       "      <td>Good morning. How are you?(Laughter)It's been ...</td>\n",
       "      <td>good morning  how are you  it s been great  ha...</td>\n",
       "      <td>[good, morning, how, are, you, it, s, been, gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>265</td>\n",
       "      <td>With the same humor and humanity he exuded in ...</td>\n",
       "      <td>977</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>1140825600</td>\n",
       "      <td>43</td>\n",
       "      <td>Al Gore</td>\n",
       "      <td>Al Gore: Averting the climate crisis</td>\n",
       "      <td>1</td>\n",
       "      <td>1151367060</td>\n",
       "      <td>[{'id': 7, 'name': 'Funny', 'count': 544}, {'i...</td>\n",
       "      <td>[{'id': 243, 'hero': 'https://pe.tedcdn.com/im...</td>\n",
       "      <td>Climate advocate</td>\n",
       "      <td>['alternative energy', 'cars', 'climate change...</td>\n",
       "      <td>Averting the climate crisis</td>\n",
       "      <td>https://www.ted.com/talks/al_gore_on_averting_...</td>\n",
       "      <td>3200520</td>\n",
       "      <td>Thank you so much, Chris. And it's truly a gre...</td>\n",
       "      <td>thank you so much  chris  and it s truly a gre...</td>\n",
       "      <td>[thank, you, so, much, chris, and, it, s, trul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>124</td>\n",
       "      <td>New York Times columnist David Pogue takes aim...</td>\n",
       "      <td>1286</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>1140739200</td>\n",
       "      <td>26</td>\n",
       "      <td>David Pogue</td>\n",
       "      <td>David Pogue: Simplicity sells</td>\n",
       "      <td>1</td>\n",
       "      <td>1151367060</td>\n",
       "      <td>[{'id': 7, 'name': 'Funny', 'count': 964}, {'i...</td>\n",
       "      <td>[{'id': 1725, 'hero': 'https://pe.tedcdn.com/i...</td>\n",
       "      <td>Technology columnist</td>\n",
       "      <td>['computers', 'entertainment', 'interface desi...</td>\n",
       "      <td>Simplicity sells</td>\n",
       "      <td>https://www.ted.com/talks/david_pogue_says_sim...</td>\n",
       "      <td>1636292</td>\n",
       "      <td>(Music: \"The Sound of Silence,\" Simon &amp; Garfun...</td>\n",
       "      <td>hello voice mail  my old friend  i ve called ...</td>\n",
       "      <td>[hello, voice, mail, my, old, friend, i, ve, c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   comments                                        description  duration  \\\n",
       "0      4553  Sir Ken Robinson makes an entertaining and pro...      1164   \n",
       "1       265  With the same humor and humanity he exuded in ...       977   \n",
       "2       124  New York Times columnist David Pogue takes aim...      1286   \n",
       "\n",
       "     event   film_date  languages  main_speaker  \\\n",
       "0  TED2006  1140825600         60  Ken Robinson   \n",
       "1  TED2006  1140825600         43       Al Gore   \n",
       "2  TED2006  1140739200         26   David Pogue   \n",
       "\n",
       "                                        name  num_speaker  published_date  \\\n",
       "0  Ken Robinson: Do schools kill creativity?            1      1151367060   \n",
       "1       Al Gore: Averting the climate crisis            1      1151367060   \n",
       "2              David Pogue: Simplicity sells            1      1151367060   \n",
       "\n",
       "                                             ratings  \\\n",
       "0  [{'id': 7, 'name': 'Funny', 'count': 19645}, {...   \n",
       "1  [{'id': 7, 'name': 'Funny', 'count': 544}, {'i...   \n",
       "2  [{'id': 7, 'name': 'Funny', 'count': 964}, {'i...   \n",
       "\n",
       "                                       related_talks    speaker_occupation  \\\n",
       "0  [{'id': 865, 'hero': 'https://pe.tedcdn.com/im...       Author/educator   \n",
       "1  [{'id': 243, 'hero': 'https://pe.tedcdn.com/im...      Climate advocate   \n",
       "2  [{'id': 1725, 'hero': 'https://pe.tedcdn.com/i...  Technology columnist   \n",
       "\n",
       "                                                tags  \\\n",
       "0  ['children', 'creativity', 'culture', 'dance',...   \n",
       "1  ['alternative energy', 'cars', 'climate change...   \n",
       "2  ['computers', 'entertainment', 'interface desi...   \n",
       "\n",
       "                         title  \\\n",
       "0  Do schools kill creativity?   \n",
       "1  Averting the climate crisis   \n",
       "2             Simplicity sells   \n",
       "\n",
       "                                                 url     views  \\\n",
       "0  https://www.ted.com/talks/ken_robinson_says_sc...  47227110   \n",
       "1  https://www.ted.com/talks/al_gore_on_averting_...   3200520   \n",
       "2  https://www.ted.com/talks/david_pogue_says_sim...   1636292   \n",
       "\n",
       "                                          transcript  \\\n",
       "0  Good morning. How are you?(Laughter)It's been ...   \n",
       "1  Thank you so much, Chris. And it's truly a gre...   \n",
       "2  (Music: \"The Sound of Silence,\" Simon & Garfun...   \n",
       "\n",
       "                                                text  \\\n",
       "0  good morning  how are you  it s been great  ha...   \n",
       "1  thank you so much  chris  and it s truly a gre...   \n",
       "2   hello voice mail  my old friend  i ve called ...   \n",
       "\n",
       "                                      tokenized_text  \n",
       "0  [good, morning, how, are, you, it, s, been, gr...  \n",
       "1  [thank, you, so, much, chris, and, it, s, trul...  \n",
       "2  [hello, voice, mail, my, old, friend, i, ve, c...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['tokenized_text'] = df_merged['tokenized_text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "\n",
    "df_merged['lemmatized_text'] = df_merged['tokenized_text'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>description</th>\n",
       "      <th>duration</th>\n",
       "      <th>event</th>\n",
       "      <th>film_date</th>\n",
       "      <th>languages</th>\n",
       "      <th>main_speaker</th>\n",
       "      <th>name</th>\n",
       "      <th>num_speaker</th>\n",
       "      <th>published_date</th>\n",
       "      <th>...</th>\n",
       "      <th>related_talks</th>\n",
       "      <th>speaker_occupation</th>\n",
       "      <th>tags</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>views</th>\n",
       "      <th>transcript</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>32</td>\n",
       "      <td>In an unmissable talk about race and politics ...</td>\n",
       "      <td>1100</td>\n",
       "      <td>TEDxMileHigh</td>\n",
       "      <td>1499472000</td>\n",
       "      <td>1</td>\n",
       "      <td>Theo E.J. Wilson</td>\n",
       "      <td>Theo E.J. Wilson: A black man goes undercover ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1506024042</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'id': 2512, 'hero': 'https://pe.tedcdn.com/i...</td>\n",
       "      <td>Public intellectual</td>\n",
       "      <td>['Internet', 'TEDx', 'United States', 'communi...</td>\n",
       "      <td>A black man goes undercover in the alt-right</td>\n",
       "      <td>https://www.ted.com/talks/theo_e_j_wilson_a_bl...</td>\n",
       "      <td>419309</td>\n",
       "      <td>I took a cell phone and accidentally made myse...</td>\n",
       "      <td>i took a cell phone and accidentally made myse...</td>\n",
       "      <td>[took, cell, phone, accidentally, made, famous...</td>\n",
       "      <td>[took, cell, phone, accidentally, made, famous...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>8</td>\n",
       "      <td>With more than half of the world population li...</td>\n",
       "      <td>519</td>\n",
       "      <td>TED2017</td>\n",
       "      <td>1492992000</td>\n",
       "      <td>1</td>\n",
       "      <td>Karoliina Korppoo</td>\n",
       "      <td>Karoliina Korppoo: How a video game might help...</td>\n",
       "      <td>1</td>\n",
       "      <td>1506092422</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'id': 2682, 'hero': 'https://pe.tedcdn.com/i...</td>\n",
       "      <td>Game designer</td>\n",
       "      <td>['cities', 'design', 'future', 'infrastructure...</td>\n",
       "      <td>How a video game might help us build better ci...</td>\n",
       "      <td>https://www.ted.com/talks/karoliina_korppoo_ho...</td>\n",
       "      <td>391721</td>\n",
       "      <td>We humans are becoming an urban species, so ci...</td>\n",
       "      <td>we humans are becoming an urban species  so ci...</td>\n",
       "      <td>[humans, becoming, urban, species, cities, nat...</td>\n",
       "      <td>[human, becoming, urban, specie, city, natural...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      comments                                        description  duration  \\\n",
       "2465        32  In an unmissable talk about race and politics ...      1100   \n",
       "2466         8  With more than half of the world population li...       519   \n",
       "\n",
       "             event   film_date  languages       main_speaker  \\\n",
       "2465  TEDxMileHigh  1499472000          1   Theo E.J. Wilson   \n",
       "2466       TED2017  1492992000          1  Karoliina Korppoo   \n",
       "\n",
       "                                                   name  num_speaker  \\\n",
       "2465  Theo E.J. Wilson: A black man goes undercover ...            1   \n",
       "2466  Karoliina Korppoo: How a video game might help...            1   \n",
       "\n",
       "      published_date  ...                                      related_talks  \\\n",
       "2465      1506024042  ...  [{'id': 2512, 'hero': 'https://pe.tedcdn.com/i...   \n",
       "2466      1506092422  ...  [{'id': 2682, 'hero': 'https://pe.tedcdn.com/i...   \n",
       "\n",
       "       speaker_occupation                                               tags  \\\n",
       "2465  Public intellectual  ['Internet', 'TEDx', 'United States', 'communi...   \n",
       "2466        Game designer  ['cities', 'design', 'future', 'infrastructure...   \n",
       "\n",
       "                                                  title  \\\n",
       "2465       A black man goes undercover in the alt-right   \n",
       "2466  How a video game might help us build better ci...   \n",
       "\n",
       "                                                    url   views  \\\n",
       "2465  https://www.ted.com/talks/theo_e_j_wilson_a_bl...  419309   \n",
       "2466  https://www.ted.com/talks/karoliina_korppoo_ho...  391721   \n",
       "\n",
       "                                             transcript  \\\n",
       "2465  I took a cell phone and accidentally made myse...   \n",
       "2466  We humans are becoming an urban species, so ci...   \n",
       "\n",
       "                                                   text  \\\n",
       "2465  i took a cell phone and accidentally made myse...   \n",
       "2466  we humans are becoming an urban species  so ci...   \n",
       "\n",
       "                                         tokenized_text  \\\n",
       "2465  [took, cell, phone, accidentally, made, famous...   \n",
       "2466  [humans, becoming, urban, species, cities, nat...   \n",
       "\n",
       "                                        lemmatized_text  \n",
       "2465  [took, cell, phone, accidentally, made, famous...  \n",
       "2466  [human, becoming, urban, specie, city, natural...  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "join lemmatized words to string for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['lemma_text_string'] = df_merged['lemmatized_text'].apply(', '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_topics_token = pipe.transform(df_merged['tokenized_text']) \n",
    "# df_topics_token = pd.DataFrame(df_topics_token, columns=[str(t_words[i]) for i in range(0,10)])\n",
    "# df_topics_token.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, number_top_words):\n",
    "        for ix, topic in enumerate(model.components_):\n",
    "            print(\"Topic \", ix)\n",
    "            print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-number_top_words - 1:-1]]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_topics(model, feature_names, number_top_words):\n",
    "    result = list()\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "#         result.append(\" \".join([feature_names[i]\n",
    "#                     for i in topic.argsort()[:-number_top_words - 1:-1]]))\n",
    "        result.append([feature_names[i]\n",
    "                    for i in topic.argsort()[:-number_top_words - 1:-1]])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_topic_mod_lda(data = df_merged['lemma_text_string'], topics = 5,\n",
    "                       iters = 10, ngram_min = 1,\n",
    "                       ngram_max = 3, max_df=0.35,\n",
    "                       min_df = 0.1,\n",
    "                       max_feats=5000, number_top_words = 20,\n",
    "                       seed = 0):\n",
    "    \n",
    "    \"\"\" \n",
    "    vectorizer - turn words into numbers for each document\n",
    "    use Latent Dirichlet Allocation to produce topics\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    vectorizer = CountVectorizer(ngram_range = (ngram_min , ngram_max), \n",
    "                             stop_words ='english', \n",
    "                             max_df = max_df, \n",
    "                             max_features = max_feats)\n",
    "    \n",
    "#     vectorizer = TfidfVectorizer(stop_words=\"english\",\n",
    "# #                         use_idf=True,\n",
    "#                         ngram_range = (ngram_min , ngram_max),\n",
    "#                         min_df = min_df,\n",
    "                             \n",
    "#                         max_df = max_df,\n",
    "#                         max_features = max_feats,\n",
    "#                         )  \n",
    "    \n",
    "    \n",
    "    #  `fit (train), then transform` to convert text to a bag of words\n",
    "\n",
    "    vect_data = vectorizer.fit_transform(data)\n",
    "    \n",
    "    \n",
    "    \n",
    "    lda = LatentDirichletAllocation(n_components = topics,\n",
    "                                    max_iter = iters,\n",
    "                                    random_state = seed,\n",
    "                                    learning_method = 'online',\n",
    "                                    n_jobs =- 1,\n",
    "                                    )\n",
    "    \n",
    "    lda_data = lda.fit_transform(vect_data)\n",
    "    \n",
    "    display_topics(lda, vectorizer.get_feature_names(), number_top_words)\n",
    "    \n",
    "    return vectorizer, vect_data, lda, lda_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic  0\n",
      "social example word society culture understand language believe sense group course century value reason god individual answer history science wrong\n",
      "Topic  1\n",
      "cell brain cancer body gene dna neuron genome blood molecule technology animal disease tissue genetic lab tumor bacteria protein biology\n",
      "Topic  2\n",
      "data computer technology information machine student language word using digital learning algorithm number example book code software tool learn paper\n",
      "Topic  3\n",
      "sound music play yeah ok oh hand hear song love voice yes feel body dance try minute guy audience okay\n",
      "Topic  4\n",
      "child family old mother home told story father wanted year old friend man knew love help young took parent learned saw\n",
      "Topic  5\n",
      "feel experience brain choice mind self feeling study happiness love decision le fear emotion happy mental sleep moment memory percent\n",
      "Topic  6\n",
      "image light art object sort design piece looking eye color camera video picture visual artist hand using create face real\n",
      "Topic  7\n",
      "patient health disease doctor drug care medical treatment hospital medicine baby heart percent child death data surgery hiv case research\n",
      "Topic  8\n",
      "guy sort stuff phone internet pretty somebody maybe couple getting looking person basically friend trying wanted little bit interesting online week\n",
      "Topic  9\n",
      "story book film movie art tell story bee character picture man photograph image read medium camera maybe live audience narrative artist\n",
      "Topic  10\n",
      "country africa percent state global china economy economic government african growth million poor poverty billion social population income aid united\n",
      "Topic  11\n",
      "food water plant climate energy oil carbon forest percent tree climate change natural specie eat waste nature fuel gas grow farmer\n",
      "Topic  12\n",
      "robot game machine rule play theory science physic ant pattern video force real intelligence build law consciousness reality quantum universe\n",
      "Topic  13\n",
      "war government state power country political law ca american united police group security democracy violence united state military conflict election em\n",
      "Topic  14\n",
      "company car dollar money business cost percent product market technology million industry buy value innovation job pay example power price\n",
      "Topic  15\n",
      "school kid child teacher student education india class high parent high school college teach classroom learning learn old young grade chinese\n",
      "Topic  16\n",
      "city building design space project community built build street new york york create public house architecture map live urban open neighborhood\n",
      "Topic  17\n",
      "woman men girl black sex female man love male white boy compassion gender young race gay sexual men woman culture violence\n",
      "Topic  18\n",
      "animal ocean water sea specie foot fish ice earth surface bird mile coral river area year ago mountain shark island planet\n",
      "Topic  19\n",
      "earth planet universe space energy light star solar sun mar galaxy billion nuclear cloud air hole telescope atmosphere black science\n"
     ]
    }
   ],
   "source": [
    "vectorizer_lda, vect_data, lda_model_lemma, lda_data_lemma = make_topic_mod_lda(data = df_merged['lemma_text_string'],\n",
    "                                    topics=20,\n",
    "                                    iters=100,\n",
    "                                    ngram_min=1, \n",
    "                                    ngram_max=2, \n",
    "                                    max_df=0.5, \n",
    "                                    min_df=0.1,\n",
    "                                    max_feats=2000\n",
    "                                                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.215427</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.111963</td>\n",
       "      <td>0.293819</td>\n",
       "      <td>0.025097</td>\n",
       "      <td>0.059096</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.032912</td>\n",
       "      <td>0.021468</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.014040</td>\n",
       "      <td>0.040530</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.145574</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.030980</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.008427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.025284</td>\n",
       "      <td>0.065837</td>\n",
       "      <td>0.016966</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.163620</td>\n",
       "      <td>0.070849</td>\n",
       "      <td>0.045031</td>\n",
       "      <td>0.218815</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.142423</td>\n",
       "      <td>0.216788</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.033303</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.245398</td>\n",
       "      <td>0.145348</td>\n",
       "      <td>0.030065</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.059229</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.317471</td>\n",
       "      <td>0.007446</td>\n",
       "      <td>0.017291</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.026418</td>\n",
       "      <td>0.150549</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.012248</td>\n",
       "      <td>0.122456</td>\n",
       "      <td>0.044513</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.165733</td>\n",
       "      <td>0.100597</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.065610</td>\n",
       "      <td>0.086197</td>\n",
       "      <td>0.010363</td>\n",
       "      <td>0.355825</td>\n",
       "      <td>0.035859</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.131950</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.041170</td>\n",
       "      <td>0.027436</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.752093</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.046268</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.215427  0.000084  0.000084  0.111963  0.293819  0.025097  0.059096   \n",
       "1  0.000108  0.000108  0.000108  0.025284  0.065837  0.016966  0.000108   \n",
       "2  0.000071  0.000071  0.245398  0.145348  0.030065  0.000071  0.059229   \n",
       "3  0.000060  0.000060  0.000060  0.012248  0.122456  0.044513  0.000060   \n",
       "4  0.000072  0.000072  0.131950  0.000072  0.000072  0.000072  0.041170   \n",
       "\n",
       "         7         8         9         10        11        12        13  \\\n",
       "0  0.000084  0.032912  0.021468  0.000084  0.014040  0.040530  0.000084   \n",
       "1  0.000108  0.163620  0.070849  0.045031  0.218815  0.000108  0.142423   \n",
       "2  0.000071  0.317471  0.007446  0.017291  0.000071  0.000071  0.026418   \n",
       "3  0.000060  0.000060  0.000060  0.165733  0.100597  0.000060  0.065610   \n",
       "4  0.027436  0.000072  0.000072  0.752093  0.000072  0.000072  0.000072   \n",
       "\n",
       "         14        15        16        17        18        19  \n",
       "0  0.000084  0.145574  0.000084  0.030980  0.000084  0.008427  \n",
       "1  0.216788  0.000108  0.033303  0.000108  0.000108  0.000108  \n",
       "2  0.150549  0.000071  0.000071  0.000071  0.000071  0.000071  \n",
       "3  0.086197  0.010363  0.355825  0.035859  0.000060  0.000060  \n",
       "4  0.000072  0.046268  0.000072  0.000072  0.000072  0.000072  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(lda_data_lemma).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_ind = np.argmax(lda_data_lemma, axis=1)\n",
    "topic_ind.shape\n",
    "y = topic_ind\n",
    "\n",
    "# create text labels for plotting\n",
    "tsne_labels = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_names = tsne_labels\n",
    "# topic_names[topic_names==0] = \"History\"\n",
    "# topic_names[topic_names==1] = \"Medicine, Vaccines, Global Health\"\n",
    "# topic_names[topic_names==2] = \"Education\"\n",
    "# topic_names[topic_names==3] = \"Family\"\n",
    "# topic_names[topic_names==4] = \"Politics, War\"\n",
    "# topic_names[topic_names==5] = \"Technolgy\"\n",
    "# topic_names[topic_names==6] = \"Gender\"\n",
    "# topic_names[topic_names==7] = \"Astronomy, Quantum Physics\"\n",
    "# topic_names[topic_names==8] = \"Machine Learning, AI\"\n",
    "\n",
    "# topic_names[topic_names==9] = \"Gaming, Music, Video\"\n",
    "# topic_names[topic_names==10] = \"Tech, Business\"\n",
    "# topic_names[topic_names==11] = \"Biology, Genetics\"\n",
    "\n",
    "# topic_names[topic_names==12] = \"Medicine, Healthcare\"\n",
    "# topic_names[topic_names==13] = \"Energy, Transportation, Climate Change\"\n",
    "\n",
    "# topic_names[topic_names==14] = \"Astronomy, Space Travel\"\n",
    "# topic_names[topic_names==15] = \"Art, Language, Literature\"  \n",
    "# topic_names[topic_names==16] = \"Environmentalism, Oceans\"\n",
    "# topic_names[topic_names==17] = \"Mindfulness, Culture, Self Care\"\n",
    "# topic_names[topic_names==18] = \"Urban Dev, Architecture\"\n",
    "# topic_names[topic_names==19] = \"Economy, Global Econ, Development\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_lda = create_topics(lda_model_lemma, vectorizer_lda.get_feature_names(), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['social',\n",
       "  'example',\n",
       "  'word',\n",
       "  'society',\n",
       "  'culture',\n",
       "  'understand',\n",
       "  'language',\n",
       "  'believe',\n",
       "  'sense',\n",
       "  'group',\n",
       "  'course',\n",
       "  'century',\n",
       "  'value',\n",
       "  'reason',\n",
       "  'god',\n",
       "  'individual',\n",
       "  'answer',\n",
       "  'history',\n",
       "  'science',\n",
       "  'wrong'],\n",
       " ['cell',\n",
       "  'brain',\n",
       "  'cancer',\n",
       "  'body',\n",
       "  'gene',\n",
       "  'dna',\n",
       "  'neuron',\n",
       "  'genome',\n",
       "  'blood',\n",
       "  'molecule',\n",
       "  'technology',\n",
       "  'animal',\n",
       "  'disease',\n",
       "  'tissue',\n",
       "  'genetic',\n",
       "  'lab',\n",
       "  'tumor',\n",
       "  'bacteria',\n",
       "  'protein',\n",
       "  'biology'],\n",
       " ['data',\n",
       "  'computer',\n",
       "  'technology',\n",
       "  'information',\n",
       "  'machine',\n",
       "  'student',\n",
       "  'language',\n",
       "  'word',\n",
       "  'using',\n",
       "  'digital',\n",
       "  'learning',\n",
       "  'algorithm',\n",
       "  'number',\n",
       "  'example',\n",
       "  'book',\n",
       "  'code',\n",
       "  'software',\n",
       "  'tool',\n",
       "  'learn',\n",
       "  'paper'],\n",
       " ['sound',\n",
       "  'music',\n",
       "  'play',\n",
       "  'yeah',\n",
       "  'ok',\n",
       "  'oh',\n",
       "  'hand',\n",
       "  'hear',\n",
       "  'song',\n",
       "  'love',\n",
       "  'voice',\n",
       "  'yes',\n",
       "  'feel',\n",
       "  'body',\n",
       "  'dance',\n",
       "  'try',\n",
       "  'minute',\n",
       "  'guy',\n",
       "  'audience',\n",
       "  'okay'],\n",
       " ['child',\n",
       "  'family',\n",
       "  'old',\n",
       "  'mother',\n",
       "  'home',\n",
       "  'told',\n",
       "  'story',\n",
       "  'father',\n",
       "  'wanted',\n",
       "  'year old',\n",
       "  'friend',\n",
       "  'man',\n",
       "  'knew',\n",
       "  'love',\n",
       "  'help',\n",
       "  'young',\n",
       "  'took',\n",
       "  'parent',\n",
       "  'learned',\n",
       "  'saw'],\n",
       " ['feel',\n",
       "  'experience',\n",
       "  'brain',\n",
       "  'choice',\n",
       "  'mind',\n",
       "  'self',\n",
       "  'feeling',\n",
       "  'study',\n",
       "  'happiness',\n",
       "  'love',\n",
       "  'decision',\n",
       "  'le',\n",
       "  'fear',\n",
       "  'emotion',\n",
       "  'happy',\n",
       "  'mental',\n",
       "  'sleep',\n",
       "  'moment',\n",
       "  'memory',\n",
       "  'percent'],\n",
       " ['image',\n",
       "  'light',\n",
       "  'art',\n",
       "  'object',\n",
       "  'sort',\n",
       "  'design',\n",
       "  'piece',\n",
       "  'looking',\n",
       "  'eye',\n",
       "  'color',\n",
       "  'camera',\n",
       "  'video',\n",
       "  'picture',\n",
       "  'visual',\n",
       "  'artist',\n",
       "  'hand',\n",
       "  'using',\n",
       "  'create',\n",
       "  'face',\n",
       "  'real'],\n",
       " ['patient',\n",
       "  'health',\n",
       "  'disease',\n",
       "  'doctor',\n",
       "  'drug',\n",
       "  'care',\n",
       "  'medical',\n",
       "  'treatment',\n",
       "  'hospital',\n",
       "  'medicine',\n",
       "  'baby',\n",
       "  'heart',\n",
       "  'percent',\n",
       "  'child',\n",
       "  'death',\n",
       "  'data',\n",
       "  'surgery',\n",
       "  'hiv',\n",
       "  'case',\n",
       "  'research'],\n",
       " ['guy',\n",
       "  'sort',\n",
       "  'stuff',\n",
       "  'phone',\n",
       "  'internet',\n",
       "  'pretty',\n",
       "  'somebody',\n",
       "  'maybe',\n",
       "  'couple',\n",
       "  'getting',\n",
       "  'looking',\n",
       "  'person',\n",
       "  'basically',\n",
       "  'friend',\n",
       "  'trying',\n",
       "  'wanted',\n",
       "  'little bit',\n",
       "  'interesting',\n",
       "  'online',\n",
       "  'week'],\n",
       " ['story',\n",
       "  'book',\n",
       "  'film',\n",
       "  'movie',\n",
       "  'art',\n",
       "  'tell story',\n",
       "  'bee',\n",
       "  'character',\n",
       "  'picture',\n",
       "  'man',\n",
       "  'photograph',\n",
       "  'image',\n",
       "  'read',\n",
       "  'medium',\n",
       "  'camera',\n",
       "  'maybe',\n",
       "  'live',\n",
       "  'audience',\n",
       "  'narrative',\n",
       "  'artist'],\n",
       " ['country',\n",
       "  'africa',\n",
       "  'percent',\n",
       "  'state',\n",
       "  'global',\n",
       "  'china',\n",
       "  'economy',\n",
       "  'economic',\n",
       "  'government',\n",
       "  'african',\n",
       "  'growth',\n",
       "  'million',\n",
       "  'poor',\n",
       "  'poverty',\n",
       "  'billion',\n",
       "  'social',\n",
       "  'population',\n",
       "  'income',\n",
       "  'aid',\n",
       "  'united'],\n",
       " ['food',\n",
       "  'water',\n",
       "  'plant',\n",
       "  'climate',\n",
       "  'energy',\n",
       "  'oil',\n",
       "  'carbon',\n",
       "  'forest',\n",
       "  'percent',\n",
       "  'tree',\n",
       "  'climate change',\n",
       "  'natural',\n",
       "  'specie',\n",
       "  'eat',\n",
       "  'waste',\n",
       "  'nature',\n",
       "  'fuel',\n",
       "  'gas',\n",
       "  'grow',\n",
       "  'farmer'],\n",
       " ['robot',\n",
       "  'game',\n",
       "  'machine',\n",
       "  'rule',\n",
       "  'play',\n",
       "  'theory',\n",
       "  'science',\n",
       "  'physic',\n",
       "  'ant',\n",
       "  'pattern',\n",
       "  'video',\n",
       "  'force',\n",
       "  'real',\n",
       "  'intelligence',\n",
       "  'build',\n",
       "  'law',\n",
       "  'consciousness',\n",
       "  'reality',\n",
       "  'quantum',\n",
       "  'universe'],\n",
       " ['war',\n",
       "  'government',\n",
       "  'state',\n",
       "  'power',\n",
       "  'country',\n",
       "  'political',\n",
       "  'law',\n",
       "  'ca',\n",
       "  'american',\n",
       "  'united',\n",
       "  'police',\n",
       "  'group',\n",
       "  'security',\n",
       "  'democracy',\n",
       "  'violence',\n",
       "  'united state',\n",
       "  'military',\n",
       "  'conflict',\n",
       "  'election',\n",
       "  'em'],\n",
       " ['company',\n",
       "  'car',\n",
       "  'dollar',\n",
       "  'money',\n",
       "  'business',\n",
       "  'cost',\n",
       "  'percent',\n",
       "  'product',\n",
       "  'market',\n",
       "  'technology',\n",
       "  'million',\n",
       "  'industry',\n",
       "  'buy',\n",
       "  'value',\n",
       "  'innovation',\n",
       "  'job',\n",
       "  'pay',\n",
       "  'example',\n",
       "  'power',\n",
       "  'price'],\n",
       " ['school',\n",
       "  'kid',\n",
       "  'child',\n",
       "  'teacher',\n",
       "  'student',\n",
       "  'education',\n",
       "  'india',\n",
       "  'class',\n",
       "  'high',\n",
       "  'parent',\n",
       "  'high school',\n",
       "  'college',\n",
       "  'teach',\n",
       "  'classroom',\n",
       "  'learning',\n",
       "  'learn',\n",
       "  'old',\n",
       "  'young',\n",
       "  'grade',\n",
       "  'chinese'],\n",
       " ['city',\n",
       "  'building',\n",
       "  'design',\n",
       "  'space',\n",
       "  'project',\n",
       "  'community',\n",
       "  'built',\n",
       "  'build',\n",
       "  'street',\n",
       "  'new york',\n",
       "  'york',\n",
       "  'create',\n",
       "  'public',\n",
       "  'house',\n",
       "  'architecture',\n",
       "  'map',\n",
       "  'live',\n",
       "  'urban',\n",
       "  'open',\n",
       "  'neighborhood'],\n",
       " ['woman',\n",
       "  'men',\n",
       "  'girl',\n",
       "  'black',\n",
       "  'sex',\n",
       "  'female',\n",
       "  'man',\n",
       "  'love',\n",
       "  'male',\n",
       "  'white',\n",
       "  'boy',\n",
       "  'compassion',\n",
       "  'gender',\n",
       "  'young',\n",
       "  'race',\n",
       "  'gay',\n",
       "  'sexual',\n",
       "  'men woman',\n",
       "  'culture',\n",
       "  'violence'],\n",
       " ['animal',\n",
       "  'ocean',\n",
       "  'water',\n",
       "  'sea',\n",
       "  'specie',\n",
       "  'foot',\n",
       "  'fish',\n",
       "  'ice',\n",
       "  'earth',\n",
       "  'surface',\n",
       "  'bird',\n",
       "  'mile',\n",
       "  'coral',\n",
       "  'river',\n",
       "  'area',\n",
       "  'year ago',\n",
       "  'mountain',\n",
       "  'shark',\n",
       "  'island',\n",
       "  'planet'],\n",
       " ['earth',\n",
       "  'planet',\n",
       "  'universe',\n",
       "  'space',\n",
       "  'energy',\n",
       "  'light',\n",
       "  'star',\n",
       "  'solar',\n",
       "  'sun',\n",
       "  'mar',\n",
       "  'galaxy',\n",
       "  'billion',\n",
       "  'nuclear',\n",
       "  'cloud',\n",
       "  'air',\n",
       "  'hole',\n",
       "  'telescope',\n",
       "  'atmosphere',\n",
       "  'black',\n",
       "  'science']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0\n",
       "0   4\n",
       "1  11\n",
       "2   8\n",
       "3  16\n",
       "4  10\n",
       "5   5\n",
       "6   4"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsne_labels.head(7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       ['children', 'creativity', 'culture', 'dance',...\n",
       "1       ['alternative energy', 'cars', 'climate change...\n",
       "2       ['computers', 'entertainment', 'interface desi...\n",
       "3       ['MacArthur grant', 'activism', 'business', 'c...\n",
       "4       ['Africa', 'Asia', 'Google', 'demo', 'economic...\n",
       "5       ['business', 'culture', 'entertainment', 'goal...\n",
       "6       ['Christianity', 'God', 'atheism', 'comedy', '...\n",
       "7       ['architecture', 'collaboration', 'culture', '...\n",
       "8       ['God', 'TED Brain Trust', 'atheism', 'brain',...\n",
       "9       ['Christianity', 'God', 'culture', 'happiness'...\n",
       "10      ['activism', 'architecture', 'collaboration', ...\n",
       "11      ['TED Prize', 'art', 'culture', 'entertainment...\n",
       "12      ['TED Prize', 'collaboration', 'disease', 'ebo...\n",
       "13      ['demo', 'design', 'interface design', 'techno...\n",
       "14      ['children', 'design', 'education', 'entrepren...\n",
       "15      ['entertainment', 'music', 'performance', 'vio...\n",
       "16      ['creativity', 'entertainment', 'music', 'perf...\n",
       "17      ['MacArthur grant', 'alternative energy', 'des...\n",
       "18      ['DNA', 'biology', 'creativity', 'design', 'in...\n",
       "19      ['business', 'collaboration', 'culture', 'inve...\n",
       "20      ['business', 'collaboration', 'culture', 'glob...\n",
       "21      ['collaboration', 'comedy', 'community', 'cult...\n",
       "22      ['business', 'communication', 'community', 'cu...\n",
       "23      ['cognitive science', 'culture', 'evolution', ...\n",
       "24      ['culture', 'entertainment', 'gender', 'global...\n",
       "25      ['climate change', 'cosmos', 'culture', 'envir...\n",
       "26      ['astronomy', 'biology', 'cognitive science', ...\n",
       "27      ['business', 'cities', 'culture', 'economics',...\n",
       "28      ['business', 'choice', 'consumerism', 'culture...\n",
       "29      ['TED Brain Trust', 'brain', 'choice', 'cultur...\n",
       "                              ...                        \n",
       "2437    ['AI', 'computers', 'economics', 'future', 'hu...\n",
       "2438    ['biology', 'biomechanics', 'cancer', 'chemist...\n",
       "2439    ['Africa', 'art', 'beauty', 'creativity', 'per...\n",
       "2440    ['TEDx', 'business', 'capitalism', 'community'...\n",
       "2441    ['Africa', 'TED Fellows', 'art', 'creativity',...\n",
       "2442    ['TEDx', 'activism', 'children', 'family', 'po...\n",
       "2443    ['AI', 'education', 'intelligence', 'robots', ...\n",
       "2444    ['TEDx', 'United States', 'activism', 'democra...\n",
       "2445    ['TEDx', 'ancient world', 'archaeology', 'cons...\n",
       "2446    ['TEDx', 'community', 'humanity', 'identity', ...\n",
       "2447    ['architecture', 'art', 'cities', 'creativity'...\n",
       "2448    ['TEDx', 'security', 'social media', 'terroris...\n",
       "2449    ['algorithm', 'business', 'collaboration', 'co...\n",
       "2450    ['TED en EspaÃ±ol', 'art', 'creativity', 'desig...\n",
       "2451    ['TEDx', 'art', 'climate change', 'environment...\n",
       "2452    ['Africa', 'activism', 'cities', 'government',...\n",
       "2453    ['TEDx', 'communication', 'friendship', 'polit...\n",
       "2454    ['happiness', 'humanity', 'personal growth', '...\n",
       "2455    ['TEDx', 'business', 'corruption', 'economics'...\n",
       "2456    ['Africa', 'agriculture', 'farming', 'food', '...\n",
       "2457    ['business', 'capitalism', 'collaboration', 'e...\n",
       "2458               ['TEDx', 'physics', 'play', 'science']\n",
       "2459    ['Africa', 'activism', 'art', 'community', 'hi...\n",
       "2460    ['DNA', 'Human body', 'biology', 'data', 'entr...\n",
       "2461    ['Africa', 'agriculture', 'history', 'leadersh...\n",
       "2462    ['TED Residency', 'United States', 'community'...\n",
       "2463    ['Mars', 'South America', 'TED Fellows', 'astr...\n",
       "2464    ['AI', 'ants', 'fish', 'future', 'innovation',...\n",
       "2465    ['Internet', 'TEDx', 'United States', 'communi...\n",
       "2466    ['cities', 'design', 'future', 'infrastructure...\n",
       "Name: tags, Length: 2467, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.7916e-01, -1.2207e-01, -2.2908e-01,  3.4682e-01,  3.0810e-01,\n",
       "       -4.8354e-01, -1.2872e-01, -4.6677e-02, -3.6068e-02, -2.1610e+00,\n",
       "        2.7579e-01,  1.1901e-01,  1.6028e-01, -6.1450e-02,  1.1554e-01,\n",
       "       -3.5981e-01,  4.8871e-01,  7.9839e-02,  3.3019e-02,  2.9834e-01,\n",
       "       -9.9063e-02,  6.6332e-01,  3.3691e-01,  5.7280e-02, -2.9128e-01,\n",
       "        2.7535e-01,  5.6892e-01,  2.5413e-01, -4.3233e-02,  2.0520e-02,\n",
       "        5.0779e-02,  5.4505e-01, -4.3760e-01,  1.0415e-01, -6.3288e-01,\n",
       "        1.4536e-01,  3.9711e-01,  4.3029e-01, -1.2240e-01, -6.6137e-02,\n",
       "        1.0925e-01, -5.0684e-01,  3.9058e-01, -5.6568e-01, -4.3140e-01,\n",
       "       -1.4120e-01, -1.9887e-01,  3.1592e-01, -3.6014e-01, -4.1200e-02,\n",
       "        6.4403e-02,  8.0507e-02, -1.7772e-02, -6.5428e-01,  1.6071e-01,\n",
       "       -3.9521e-01,  1.4980e-01, -5.0789e-01, -4.1719e-01,  4.7289e-02,\n",
       "        4.4494e-01, -2.7687e-01,  2.2294e-01,  1.7580e-02, -2.1816e-01,\n",
       "       -3.1820e-01,  2.9193e-01,  6.4577e-01,  2.4003e-01, -3.7022e-01,\n",
       "       -5.5074e-01, -1.2535e-01, -1.5050e-01, -3.2044e-01,  5.8076e-02,\n",
       "       -2.6141e-01, -2.6515e-01,  2.6757e-01,  2.0522e-01,  1.1733e-01,\n",
       "       -6.2157e-02, -2.2650e-01,  3.2636e-01, -4.0547e-01,  2.4161e-01,\n",
       "       -2.5849e-01,  2.8644e-01,  2.3234e-01, -4.5893e-01,  1.5047e-01,\n",
       "       -2.5702e-01,  1.9053e-01, -1.8505e-01,  2.9386e-01,  5.3166e-01,\n",
       "       -4.1142e-01,  5.6568e-01,  7.1885e-02,  5.3228e-02, -7.7315e-01,\n",
       "        4.3245e-01, -4.2197e-01, -6.7942e-01, -3.4763e-01,  5.5123e-02,\n",
       "        2.1531e-01, -4.5282e-01,  4.2466e-02, -1.3439e-03, -5.1621e-01,\n",
       "        3.8041e-01, -7.6564e-01,  4.2542e-01,  1.8234e-01,  5.2484e-01,\n",
       "       -4.6742e-02,  4.2889e-01,  1.2920e-01, -6.1711e-01, -2.6509e-01,\n",
       "        3.4117e-01,  8.1478e-01,  1.0115e-01,  3.0683e-01,  3.1813e-01,\n",
       "       -2.4696e-01,  4.0815e-01,  1.3617e-01,  4.1390e-01, -2.0441e-01,\n",
       "       -3.5409e-02, -2.2744e-01,  1.5432e-01, -2.0123e-01, -3.6867e-01,\n",
       "       -6.2436e-01,  5.9698e-03,  2.7608e-01, -4.6925e-02,  6.6007e-01,\n",
       "       -6.5450e-01,  5.6801e-01, -8.3432e-02,  3.0107e-01,  9.9073e-01,\n",
       "        3.1901e-01,  4.4961e-01,  3.1485e-01, -1.3806e-01,  3.0476e-01,\n",
       "        7.6140e-01,  1.5937e-01, -1.5931e-01, -5.3339e-02, -3.5352e-01,\n",
       "        1.7250e-01,  3.2794e-01,  1.8236e-01,  1.5218e-01, -3.3884e-01,\n",
       "        8.7456e-03,  4.4977e-01,  3.8210e-01, -8.0203e-01, -4.3643e-01,\n",
       "       -5.1440e-02, -2.3985e-01,  1.4032e-01,  4.5397e-01,  1.7427e-01,\n",
       "       -3.2304e-02, -3.4719e-01, -8.6875e-01,  2.5163e-01,  4.1114e-01,\n",
       "       -3.7293e-01,  1.3172e-01,  4.6399e-02, -3.9607e-02,  7.1769e-01,\n",
       "        5.4314e-01,  1.2344e-01,  5.0076e-01, -1.0772e-01, -5.2271e-01,\n",
       "        6.7043e-02, -4.1180e-01, -7.4812e-02, -1.7089e-01, -2.7040e-01,\n",
       "       -8.8300e-02,  2.3628e-02, -5.0854e-01,  1.5211e-01, -3.0964e-01,\n",
       "        8.4908e-01,  4.6413e-02,  1.6211e-01,  4.6980e-01,  1.7594e-01,\n",
       "        1.7096e-01, -1.0946e-01,  1.6183e-01,  4.4360e-01, -2.5097e-01,\n",
       "       -3.3531e-01,  6.6059e-02, -2.6647e-01,  3.3417e-01,  6.3775e-01,\n",
       "        6.2871e-02, -3.2834e-01,  1.7064e-01, -1.8338e-01,  3.6736e-02,\n",
       "        4.3476e-01,  1.9022e-01,  3.3518e-01,  8.0531e-01,  6.5139e-01,\n",
       "       -3.4491e-01,  8.0955e-01,  4.7919e-02,  1.0221e-01,  7.3825e-02,\n",
       "       -3.7203e-01, -1.8671e-01, -8.2490e-03, -2.1370e-02,  1.7367e-01,\n",
       "       -1.8885e-01,  5.2529e-02,  1.6441e-01, -1.5935e-02, -2.3422e-01,\n",
       "        1.3141e-01,  4.3711e-03,  6.5476e-03, -1.1993e-01,  4.9854e-02,\n",
       "        2.5672e-01,  2.5477e-02,  1.1040e+00, -1.3283e-01, -1.4002e+00,\n",
       "        6.1265e-01,  3.7408e-01, -1.0010e-01,  2.2123e-01,  2.1231e-01,\n",
       "        1.6619e-02,  1.9977e-01,  5.3672e-01,  1.4011e-01,  1.4122e-01,\n",
       "        2.3139e-01,  2.8986e-02, -2.7415e-01,  1.4629e-02,  1.9029e-01,\n",
       "        4.3212e-02, -5.4435e-01,  8.3612e-01, -2.9038e-01, -2.3002e-01,\n",
       "        2.3775e-02, -4.7197e-01,  5.2886e-01,  6.1372e-02,  1.7747e-01,\n",
       "       -3.2086e-01,  5.7877e-01, -1.3479e-03,  1.5321e-01,  2.5178e-01,\n",
       "       -2.3725e-01, -1.6931e+00, -3.8129e-02,  1.2742e+00, -3.9827e-01,\n",
       "       -1.5659e-01,  1.3184e-01,  3.9775e-02, -2.3194e-01,  1.8847e-01,\n",
       "       -3.4044e-02, -1.2303e-01,  9.3973e-02,  3.0218e-01,  2.3077e-01,\n",
       "       -1.5970e-01, -1.6160e-01, -2.7032e-01,  1.0683e-02,  1.6078e-01,\n",
       "        3.7757e-01, -2.1858e-01,  2.7425e-02, -2.8656e-01, -3.0284e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[topics_lda[0][0]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       ['children', 'creativity', 'culture', 'dance',...\n",
       "1       ['alternative energy', 'cars', 'climate change...\n",
       "2       ['computers', 'entertainment', 'interface desi...\n",
       "3       ['MacArthur grant', 'activism', 'business', 'c...\n",
       "4       ['Africa', 'Asia', 'Google', 'demo', 'economic...\n",
       "5       ['business', 'culture', 'entertainment', 'goal...\n",
       "6       ['Christianity', 'God', 'atheism', 'comedy', '...\n",
       "7       ['architecture', 'collaboration', 'culture', '...\n",
       "8       ['God', 'TED Brain Trust', 'atheism', 'brain',...\n",
       "9       ['Christianity', 'God', 'culture', 'happiness'...\n",
       "10      ['activism', 'architecture', 'collaboration', ...\n",
       "11      ['TED Prize', 'art', 'culture', 'entertainment...\n",
       "12      ['TED Prize', 'collaboration', 'disease', 'ebo...\n",
       "13      ['demo', 'design', 'interface design', 'techno...\n",
       "14      ['children', 'design', 'education', 'entrepren...\n",
       "15      ['entertainment', 'music', 'performance', 'vio...\n",
       "16      ['creativity', 'entertainment', 'music', 'perf...\n",
       "17      ['MacArthur grant', 'alternative energy', 'des...\n",
       "18      ['DNA', 'biology', 'creativity', 'design', 'in...\n",
       "19      ['business', 'collaboration', 'culture', 'inve...\n",
       "20      ['business', 'collaboration', 'culture', 'glob...\n",
       "21      ['collaboration', 'comedy', 'community', 'cult...\n",
       "22      ['business', 'communication', 'community', 'cu...\n",
       "23      ['cognitive science', 'culture', 'evolution', ...\n",
       "24      ['culture', 'entertainment', 'gender', 'global...\n",
       "25      ['climate change', 'cosmos', 'culture', 'envir...\n",
       "26      ['astronomy', 'biology', 'cognitive science', ...\n",
       "27      ['business', 'cities', 'culture', 'economics',...\n",
       "28      ['business', 'choice', 'consumerism', 'culture...\n",
       "29      ['TED Brain Trust', 'brain', 'choice', 'cultur...\n",
       "                              ...                        \n",
       "2437    ['AI', 'computers', 'economics', 'future', 'hu...\n",
       "2438    ['biology', 'biomechanics', 'cancer', 'chemist...\n",
       "2439    ['Africa', 'art', 'beauty', 'creativity', 'per...\n",
       "2440    ['TEDx', 'business', 'capitalism', 'community'...\n",
       "2441    ['Africa', 'TED Fellows', 'art', 'creativity',...\n",
       "2442    ['TEDx', 'activism', 'children', 'family', 'po...\n",
       "2443    ['AI', 'education', 'intelligence', 'robots', ...\n",
       "2444    ['TEDx', 'United States', 'activism', 'democra...\n",
       "2445    ['TEDx', 'ancient world', 'archaeology', 'cons...\n",
       "2446    ['TEDx', 'community', 'humanity', 'identity', ...\n",
       "2447    ['architecture', 'art', 'cities', 'creativity'...\n",
       "2448    ['TEDx', 'security', 'social media', 'terroris...\n",
       "2449    ['algorithm', 'business', 'collaboration', 'co...\n",
       "2450    ['TED en EspaÃ±ol', 'art', 'creativity', 'desig...\n",
       "2451    ['TEDx', 'art', 'climate change', 'environment...\n",
       "2452    ['Africa', 'activism', 'cities', 'government',...\n",
       "2453    ['TEDx', 'communication', 'friendship', 'polit...\n",
       "2454    ['happiness', 'humanity', 'personal growth', '...\n",
       "2455    ['TEDx', 'business', 'corruption', 'economics'...\n",
       "2456    ['Africa', 'agriculture', 'farming', 'food', '...\n",
       "2457    ['business', 'capitalism', 'collaboration', 'e...\n",
       "2458               ['TEDx', 'physics', 'play', 'science']\n",
       "2459    ['Africa', 'activism', 'art', 'community', 'hi...\n",
       "2460    ['DNA', 'Human body', 'biology', 'data', 'entr...\n",
       "2461    ['Africa', 'agriculture', 'history', 'leadersh...\n",
       "2462    ['TED Residency', 'United States', 'community'...\n",
       "2463    ['Mars', 'South America', 'TED Fellows', 'astr...\n",
       "2464    ['AI', 'ants', 'fish', 'future', 'innovation',...\n",
       "2465    ['Internet', 'TEDx', 'United States', 'communi...\n",
       "2466    ['cities', 'design', 'future', 'infrastructure...\n",
       "Name: tags, Length: 2467, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.iloc[0].tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = df_merged.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tags = tags.str.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = tags.apply(punc_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = tags.apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = tags.apply(lambda x: re.sub('^\\s+|\\s+$', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = tags.apply(lambda x: re.sub(' +', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = tags.apply(lambda x: x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['children',\n",
       " 'creativity',\n",
       " 'culture',\n",
       " 'dance',\n",
       " 'education',\n",
       " 'parenting',\n",
       " 'teaching']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove words from tags that are not in word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list( range(0, 2467) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in l:\n",
    "    tags[i] = [word for word in model.vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = list()\n",
    "for i in l:\n",
    "    centroids.append(np.mean(model[tags[i]], axis=0))\n",
    "    \n",
    "centroids = np.array(centroids)\n",
    "centroids = centroids.reshape((2467, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.2382111e-02, -8.3333418e-02, -4.7429472e-05, ...,\n",
       "         2.1872786e-01,  1.8184374e-01, -6.7022957e-02],\n",
       "       [ 9.2382111e-02, -8.3333418e-02, -4.7429472e-05, ...,\n",
       "         2.1872786e-01,  1.8184374e-01, -6.7022957e-02],\n",
       "       [ 9.2382111e-02, -8.3333418e-02, -4.7429472e-05, ...,\n",
       "         2.1872786e-01,  1.8184374e-01, -6.7022957e-02],\n",
       "       ...,\n",
       "       [ 9.2382111e-02, -8.3333418e-02, -4.7429472e-05, ...,\n",
       "         2.1872786e-01,  1.8184374e-01, -6.7022957e-02],\n",
       "       [ 9.2382111e-02, -8.3333418e-02, -4.7429472e-05, ...,\n",
       "         2.1872786e-01,  1.8184374e-01, -6.7022957e-02],\n",
       "       [ 9.2382111e-02, -8.3333418e-02, -4.7429472e-05, ...,\n",
       "         2.1872786e-01,  1.8184374e-01, -6.7022957e-02]], dtype=float32)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
